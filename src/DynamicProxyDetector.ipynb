{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "88dc48cd-424e-4000-9fec-2a361449c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%run ./BaseProxyDetector.ipynb\n",
    "%run ./GoogleClient.ipynb\n",
    "%run ./Utils.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0e32b951-9c29-4bcb-9dbc-d8e68419837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicProxyDetector(BaseProxyDetector, GoogleClient):\n",
    "\n",
    "    \"\"\"\n",
    "    A class to detect proxy contracts in a dynamic way. For more details about the study, please refer to \n",
    "        @article{Ebrahimi23,\n",
    "        author = {Ebrahimi, Amir and Adams, Bram and Oliva, Gustavo and Hassan, Ahmed E.},\n",
    "        year = {2023},\n",
    "        month = {05},\n",
    "        pages = {},\n",
    "        title = {A Large-Scale Exploratory Study on the Proxy Pattern in Ethereum}\n",
    "        }  \n",
    "        \n",
    "    Attributes:\n",
    "        project_id (str): Google Cloud project ID where the dataset and tables reside.\n",
    "        dataset_name (str): The name of the dataset to operate within.\n",
    "        client: The Google BigQuery client\n",
    "    \"\"\"\n",
    "    def __init__(self, json_credentials, project_id, dataset_name, storage_path = Utils.DATA_DIR, init=False):\n",
    "\n",
    "        super().__init__(json_credentials, project_id, dataset_name, storage_path)\n",
    "        \n",
    "        # Define table names for storing processed data\n",
    "        self.contracts_table_name = \"df-contracts\"    \n",
    "        self.contracts_delegate_trace_table_name = \"df-contracts-delegate-traces-table\"\n",
    "        self.proxy_logic_pairs_table_name = \"df-proxy-logic-pairs\"\n",
    "        self.contracts_proxy_status_table_name = \"df-contracts-proxy-status\"\n",
    "        self.active_proxy_contracts_table_name = \"df-active-proxy-contracts\"\n",
    "        if init:\n",
    "            self.identify_proxies_as_of(as_of_timestamp=Utils.STUDY_END_DATE)\n",
    "\n",
    "        self.active_proxy_logic_pairs = self.load_active_proxy_logic_pairs()\n",
    "    \n",
    "    def aggregate_delegatecall_traces(self, contracts_table, table_name, as_of_timestamp = '2040-09-01', override=False):\n",
    "        \"\"\"\n",
    "        Collect full traces for contracts that delegate to another contract.\n",
    "\n",
    "        Args:\n",
    "            contracts_table (str): Name of the table containing contract addresses.\n",
    "            table_name (str): Name of the table to create for storing traces.\n",
    "            override (bool): If True, the existing table with the same name will be overwritten.\n",
    "            as_of_timestamp (str): The timestamp as of traces has to be collected\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If any parameter is missing or incorrect.\n",
    "            ValueError: If dataset not fund under the project\n",
    "            ValueError: If contracts_table not fund under the project\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate inputs\n",
    "        if not all(isinstance(param, str) and param for param in [contracts_table, self.project_id, self.dataset_name, as_of_timestamp, table_name]):\n",
    "            raise ValueError(\"contracts_table, project_id, dataset_name, and table_name must be non-empty strings.\")\n",
    "        if not self.dataset_exists():\n",
    "            raise ValueError(f\"The '{self.dataset_name}' dataset does not exist in project '{self.project_id}'.\")\n",
    "        if not self.table_exists(contracts_table):\n",
    "            raise ValueError(f\"The '{contracts_table}' table does not exist under project '{self.project_id}.{self.dataset_name}'.\")\n",
    "\n",
    "        # If override is True, drop the existing table\n",
    "        if override:\n",
    "            full_table_path = f\"{self.project_id}.{self.dataset_name}.{table_name}\"\n",
    "            drop_table_query = f\"DROP TABLE IF EXISTS `{full_table_path}`\"\n",
    "            self.client.query(drop_table_query).result()  # Wait for the query to finish\n",
    "            print(f\"Existing table dropped: {full_table_path}\")\n",
    "\n",
    "        # Construct the SQL query dynamically\n",
    "        sql_query = f\"\"\"\n",
    "        CREATE TABLE `{self.project_id}.{self.dataset_name}.{table_name}` AS (\n",
    "            SELECT *,\n",
    "            REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (trace_address, \"197\", \"ÿ\"), \"196\", \"þ\"), \"195\", \"ý\"), \"194\", \"ü\"), \"193\", \"û\"), \"192\", \"ú\"), \"191\", \"ù\"), \"190\", \"ø\"), \"189\", \"÷\"), \"188\", \"ö\"), \"187\", \"õ\"), \"186\", \"ô\"), \"185\", \"ó\"), \"184\", \"ò\"), \"183\", \"ñ\"), \"182\", \"ð\"), \"181\", \"ï\"), \"180\", \"î\"), \"179\", \"í\"), \"178\", \"ì\"), \"177\", \"ë\"), \"176\", \"ê\"), \"175\", \"é\"), \"174\", \"è\"), \"173\", \"ç\"), \"172\", \"æ\"), \"171\", \"å\"), \"170\", \"ä\"), \"169\", \"ã\"), \"168\", \"â\"), \"167\", \"á\"), \"166\", \"à\"), \"165\", \"ß\"), \"164\", \"Þ\"), \"163\", \"Ý\"), \"162\", \"Ü\"), \"161\", \"Û\"), \"160\", \"Ú\"), \"159\", \"Ù\"), \"158\", \"Ø\"), \"157\", \"×\"), \"156\", \"Ö\"), \"155\", \"Õ\"), \"154\", \"Ô\"), \"153\", \"Ó\"), \"152\", \"Ò\"), \"151\", \"Ñ\"), \"150\", \"Ð\"), \"149\", \"Ï\"), \"148\", \"Î\"), \"147\", \"Í\"), \"146\", \"Ì\"), \"145\", \"Ë\"), \"144\", \"Ê\"), \"143\", \"É\"), \"142\", \"È\"), \"141\", \"Ç\"), \"140\", \"Æ\"), \"139\", \"Å\"), \"138\", \"Ä\"), \"137\", \"Ã\"), \"136\", \"Â\"), \"135\", \"Á\"), \"134\", \"À\"), \"133\", \"¿\"), \"132\", \"¾\"), \"131\", \"½\"), \"130\", \"¼\"), \"129\", \"»\"), \"128\", \"º\"), \"127\", \"¹\"), \"126\", \"¸\"), \"125\", \"·\"), \"124\", \"¶\"), \"123\", \"µ\"), \"122\", \"´\"), \"121\", \"³\"), \"120\", \"²\"), \"119\", \"±\"), \"118\", \"°\"), \"117\", \"¯\"), \"116\", \"®\"), \"115\", \"¬­\"), \"114\", \"«\"), \"113\", \"ª\"), \"112\", \"©\"), \"111\", \"¨\"), \"110\", \"§\"), \"109\", \"¦\"), \"108\", \"¥\"), \"107\", \"¤\"), \"106\", \"£\"), \"105\", \"¢\"), \"104\", \"¡\"), \"103\", \"Ÿ\"), \"102\", \"ž\"), \"101\", \"œ\"), \"100\", \"›\"), \"99\", \"š\"), \"98\", \"™\"), \"97\", \"˜\"), \"96\", \"—\"), \"95\", \"–\"), \"94\", \"•\"), \"93\", \"”\"), \"92\", \"“\"), \"91\", \"’\"), \"90\", \"‘\"), \"89\", \"Ž\"), \"88\", \"Œ\"), \"87\", \"‹\"), \"86\", \"Š\"), \"85\", \"‰\"), \"84\", \"ˆ\"), \"83\", \"‡\"), \"82\", \"†\"), \"81\", \"„\"), \"80\", \"ƒ\"), \"79\", \"€\"), \"78\", \"~\"), \"77\", \"}}\"), \"76\", \"|\"), \"75\", \"{{\"), \"74\", \"z\"), \"73\", \"y\"), \"72\", \"x\"), \"71\", \"w\"), \"70\", \"v\"), \"69\", \"u\"), \"68\", \"t\"), \"67\", \"s\"), \"66\", \"r\"), \"65\", \"q\"), \"64\", \"p\"), \"63\", \"o\"), \"62\", \"n\"), \"61\", \"m\"), \"60\", \"l\"), \"59\", \"k\"), \"58\", \"j\"), \"57\", \"i\"), \"56\", \"h\"), \"55\", \"g\"), \"54\", \"f\"), \"53\", \"e\"), \"52\", \"d\"), \"51\", \"c\"), \"50\", \"b\"), \"49\", \"a\"), \"48\", \"_\"), \"47\", \"^\"), \"46\", \"]\"), \"45\", \"[\"), \"44\", \"Z\"), \"43\", \"Y\"), \"42\", \"X\"), \"41\", \"W\"), \"40\", \"V\"), \"39\", \"U\"), \"38\", \"T\"), \"37\", \"S\"), \"36\", \"R\"), \"35\", \"Q\"), \"34\", \"P\"), \"33\", \"O\"), \"32\", \"N\"), \"31\", \"M\"), \"30\", \"L\"), \"29\", \"K\"), \"28\", \"J\"), \"27\", \"I\"), \"26\", \"H\"), \"25\", \"G\"), \"24\", \"F\"), \"23\", \"E\"), \"22\", \"D\"), \"21\", \"C\"), \"20\", \"B\"), \"19\", \"A\"), \"18\", \"@\"), \"17\", \"?\"), \"16\", \">\"), \"15\", \"=\"), \"14\", \"<\"), \"13\", \";\"), \"12\", \":\"), \"11\", \"/\"), \"10\", \".\"), \"9\", \"-\"), \"8\", \"+\"), \"7\", \"*\"), \"6\", \")\"), \"5\", \"(\"), \"4\", \"&\"), \"3\", \"%\"), \"2\", \"$\"), \"1\", \"#\"), \"0\", \"!\") AS trace_address2\n",
    "            FROM `bigquery-public-data.crypto_ethereum.traces`\n",
    "            WHERE transaction_hash IN (\n",
    "                SELECT transaction_hash\n",
    "                FROM `bigquery-public-data.crypto_ethereum.traces`\n",
    "                WHERE from_address IN (\n",
    "                    SELECT from_address\n",
    "                    FROM `{self.project_id}.{self.dataset_name}.{contracts_table}`\n",
    "                )\n",
    "                AND block_timestamp < TIMESTAMP('{as_of_timestamp}')\n",
    "                AND call_type = 'delegatecall'\n",
    "                AND status != 0\n",
    "            )\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        # Run the query to create the new table\n",
    "        query_job = self.client.query(sql_query)\n",
    "        query_job.result()  # Wait for the query to finish\n",
    "        print(f\"Table created successfully: {self.project_id}.{self.dataset_name}.{table_name}\")\n",
    "\n",
    "    def identify_proxy_logic_pairs(self, contract_delegate_traces_table, contracts_table, table_name, override=False):\n",
    "        \"\"\"\n",
    "        Detect all the proxy logic/implementation contract pairs and store the result in a new table.\n",
    "\n",
    "        Args:\n",
    "            contract_delegate_traces_table (str): Name of the table containing contract delegate traces.\n",
    "            contracts_table (str): Name of the table containing contract addresses.\n",
    "            table_name (str): Name of the table to create for storing the result.\n",
    "            override (bool): If True, the existing table with the same name will be overwritten.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If any parameter is missing or incorrect.\n",
    "            ValueError: If dataset not fund under the project\n",
    "            ValueError: If contract_delegate_traces_table not fund under the project\n",
    "            ValueError: If contracts_table not fund under the project\n",
    "        \"\"\"\n",
    "        \n",
    "        if not all(isinstance(param, str) and param for param in [contract_delegate_traces_table, contracts_table, self.project_id, self.dataset_name, table_name]):\n",
    "            raise ValueError(\"contracts_table, project_id, dataset_name, and table_name must be non-empty strings.\")\n",
    "        if not self.dataset_exists():\n",
    "            raise ValueError(f\"The '{self.dataset_name}' dataset does not exist in project '{self.project_id}'.\")\n",
    "        if not self.table_exists(contract_delegate_traces_table):\n",
    "            raise ValueError(f\"The '{contract_delegate_traces_table}' table does not exist under project '{self.project_id}.{self.dataset_name}'.\")\n",
    "        if not self.table_exists(contracts_table):\n",
    "            raise ValueError(f\"The '{contracts_table}' table does not exist under project '{self.project_id}.{self.dataset_name}'.\")\n",
    "        \n",
    "\n",
    "        # If override is True, drop the existing table\n",
    "        if override:\n",
    "            full_table_path = f\"{self.project_id}.{self.dataset_name}.{table_name}\"\n",
    "            drop_table_query = f\"DROP TABLE IF EXISTS `{full_table_path}`\"\n",
    "            self.client.query(drop_table_query).result()  # Wait for the query to finish\n",
    "            print(f\"Existing table dropped: {full_table_path}\")\n",
    "\n",
    "        # Construct the SQL query dynamically\n",
    "        sql_query = f\"\"\"\n",
    "        CREATE TABLE `{self.project_id}.{self.dataset_name}.{table_name}` AS (\n",
    "            SELECT *\n",
    "            FROM (\n",
    "                SELECT\n",
    "                    R.to_address AS from_address,\n",
    "                    L.to_address,\n",
    "                    COUNT(DISTINCT L.transaction_hash) AS counts\n",
    "                FROM (\n",
    "                    SELECT\n",
    "                        transaction_hash,\n",
    "                        from_address,\n",
    "                        to_address,\n",
    "                        input,\n",
    "                        trace_address,\n",
    "                        trace_address2\n",
    "                    FROM\n",
    "                        `{self.project_id}.{self.dataset_name}.{contract_delegate_traces_table}`\n",
    "                    WHERE\n",
    "                        call_type='delegatecall'\n",
    "                        AND status != 0 ) AS L\n",
    "                LEFT JOIN (\n",
    "                    SELECT\n",
    "                        OUTER_TRACE.input,\n",
    "                        OUTER_TRACE.from_address,\n",
    "                        OUTER_TRACE.to_address,\n",
    "                        OUTER_TRACE.transaction_hash,\n",
    "                        OUTER_TRACE.trace_address,\n",
    "                        OUTER_TRACE.trace_address2\n",
    "                    FROM\n",
    "                        `{self.project_id}.{self.dataset_name}.{contract_delegate_traces_table}` AS OUTER_TRACE\n",
    "                    RIGHT JOIN (\n",
    "                        SELECT\n",
    "                            transaction_hash\n",
    "                        FROM\n",
    "                            `{self.project_id}.{self.dataset_name}.{contract_delegate_traces_table}`\n",
    "                        GROUP BY\n",
    "                            transaction_hash\n",
    "                        HAVING\n",
    "                            MAX(subtraces) < 198) AS INNER_TRACE\n",
    "                    ON\n",
    "                        OUTER_TRACE.transaction_hash = INNER_TRACE.transaction_hash ) AS R\n",
    "                ON\n",
    "                    L.transaction_hash = R.transaction_hash\n",
    "                WHERE\n",
    "                    (R.trace_address2 < L.trace_address2\n",
    "                        OR R.trace_address IS NULL)\n",
    "                    AND COALESCE(R.trace_address,\"root\") = COALESCE(NULLIF(SUBSTRING(L.trace_address,0,CAST((ABS(LENGTH(L.trace_address)-2)+LENGTH(L.trace_address)-2)/2 AS int) ),''),\"root\")\n",
    "                    AND SUBSTRING(L.input, 1,10) = SUBSTRING(R.input, 1,10)\n",
    "                GROUP BY\n",
    "                    R.to_address,\n",
    "                    L.to_address\n",
    "                ORDER BY\n",
    "                    R.to_address )\n",
    "            WHERE\n",
    "                from_address IN (\n",
    "                    SELECT\n",
    "                        from_address\n",
    "                    FROM\n",
    "                        `{self.project_id}.{self.dataset_name}.{contracts_table}`))\n",
    "        \"\"\"\n",
    "\n",
    "        # Run the query to create the new table\n",
    "        query_job = self.client.query(sql_query)\n",
    "        query_job.result()  # Wait for the query to finish\n",
    "        print(f\"Table created successfully: {self.project_id}.{self.dataset_name}.{table_name}\")\n",
    "        \n",
    "        self.export_table_to_gcs(bucket_name = \"{}-{}\".format(self.project_id, table_name), target_table = table_name, shard_name = \"df\")\n",
    "        self.download_bucket_from_gcs(bucket_name = \"{}-{}\".format(self.project_id, table_name), output_dir_name = table_name)\n",
    "        self.decompress_gz_files(target_directory = os.path.join(self.storage_bucket_compresssed_path, table_name), output_dir_name = table_name)\n",
    "\n",
    "\n",
    "    def evaluate_active_proxy_status(self, contracts_table, proxy_logic_pairs_table, table_name, override = False):\n",
    "        \"\"\"\n",
    "        Determine the proxy status of contracts and store the result in a new table. \n",
    "        If contract is an active proxy, the is_active_proxy column will be True; otherwise, False\n",
    "\n",
    "        Args:\n",
    "            contracts_table (str): Name of the table containing contract addresses.\n",
    "            proxy_logic_pairs_table (str): Name of the table containing proxy-logic pairs.\n",
    "            table_name (str): Name of the table to create for storing the result.\n",
    "            override (bool): If True, the existing table with the same name will be overwritten.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If any parameter is missing or incorrect.\n",
    "            ValueError: If dataset not fund under the project\n",
    "            ValueError: If contracts_table not fund under the project\n",
    "            ValueError: If proxy_logic_pairs_table not fund under the project\n",
    "        \"\"\"\n",
    "        \n",
    "        if not all(isinstance(param, str) and param for param in [self.dataset_name, self.project_id, contracts_table, proxy_logic_pairs_table, table_name]):\n",
    "            raise ValueError(\"contracts_table, project_id, dataset_name, and table_name must be non-empty strings.\")\n",
    "        if not self.dataset_exists():\n",
    "            raise ValueError(f\"The '{self.dataset_name}' dataset does not exist in project '{self.project_id}'.\")\n",
    "        if not self.table_exists(contracts_table):\n",
    "            raise ValueError(f\"The '{contracts_table}' table does not exist under project '{self.project_id}.{self.dataset_name}'.\")\n",
    "        if not self.table_exists(proxy_logic_pairs_table):\n",
    "            raise ValueError(f\"The '{proxy_logic_pairs_table}' table does not exist under project '{self.project_id}.{self.dataset_name}'.\")\n",
    "      \n",
    "        # If override is True, drop the existing table\n",
    "        if override:\n",
    "            full_table_path = f\"{self.project_id}.{self.dataset_name}.{table_name}\"\n",
    "            drop_table_query = f\"DROP TABLE IF EXISTS `{full_table_path}`\"\n",
    "            self.client.query(drop_table_query).result()  # Wait for the query to finish\n",
    "            print(f\"Existing table dropped: {full_table_path}\")\n",
    "\n",
    "        # Construct the SQL query dynamically\n",
    "        sql_query = f\"\"\"\n",
    "        CREATE TABLE `{self.project_id}.{self.dataset_name}.{table_name}` AS (\n",
    "            SELECT\n",
    "                contracts.from_address,\n",
    "                COALESCE(proxy_logic_pairs.is_active, False) AS is_active_proxy\n",
    "            FROM\n",
    "                `{self.project_id}.{self.dataset_name}.{contracts_table}` AS contracts\n",
    "            LEFT JOIN (\n",
    "                SELECT\n",
    "                    DISTINCT from_address,\n",
    "                    TRUE AS is_active\n",
    "                FROM\n",
    "                    `{self.project_id}.{self.dataset_name}.{proxy_logic_pairs_table}` ) AS proxy_logic_pairs\n",
    "            ON\n",
    "                proxy_logic_pairs.from_address = contracts.from_address)\n",
    "        \"\"\"\n",
    "\n",
    "        # Run the query to create the new table\n",
    "        query_job = self.client.query(sql_query)\n",
    "        query_job.result()  # Wait for the query to finish\n",
    "        print(f\"Table created successfully: {self.project_id}.{self.dataset_name}.{table_name}\")\n",
    "\n",
    "    def download_active_proxy_contracts(self, contracts_proxy_status_table, table_name, override = False):\n",
    "        \"\"\"\n",
    "        Determine and download the list of active proxy contracts. \n",
    "        If contract is an active proxy, the is_active_proxy column will be True; otherwise, False\n",
    "\n",
    "        Args:\n",
    "            contracts_proxy_status_table (str): Name of the table containing contracts proxy status.\n",
    "            table_name (str): Name of the table to create for storing the result.\n",
    "            override (bool): If True, the existing table with the same name will be overwritten.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If any parameter is missing or incorrect.\n",
    "            ValueError: If dataset not fund under the project\n",
    "            ValueError: If contracts_proxy_status_table not fund under the project\n",
    "        \"\"\"\n",
    "        \n",
    "        if not all(isinstance(param, str) and param for param in [self.dataset_name, self.project_id, contracts_proxy_status_table, table_name]):\n",
    "            raise ValueError(\"contracts_table, project_id, dataset_name, and table_name must be non-empty strings.\")\n",
    "        if not self.dataset_exists():\n",
    "            raise ValueError(f\"The '{self.dataset_name}' dataset does not exist in project '{self.project_id}'.\")\n",
    "        if not self.table_exists(contracts_proxy_status_table):\n",
    "            raise ValueError(f\"The '{contracts_proxy_status_table}' table does not exist under project '{self.project_id}.{self.dataset_name}'.\")\n",
    "\n",
    "        # If override is True, drop the existing table\n",
    "        if override:\n",
    "            full_table_path = f\"{self.project_id}.{self.dataset_name}.{table_name}\"\n",
    "            drop_table_query = f\"DROP TABLE IF EXISTS `{full_table_path}`\"\n",
    "            self.client.query(drop_table_query).result()  # Wait for the query to finish\n",
    "            print(f\"Existing table dropped: {full_table_path}\")\n",
    "\n",
    "        # Construct the SQL query dynamically\n",
    "        sql_query = f\"\"\"\n",
    "        CREATE TABLE `{self.project_id}.{self.dataset_name}.{table_name}` AS (\n",
    "            SELECT\n",
    "                *\n",
    "            FROM\n",
    "                `{self.project_id}.{self.dataset_name}.{contracts_proxy_status_table}`\n",
    "            WHERE is_active_proxy = True)\n",
    "        \"\"\"\n",
    "\n",
    "        # Run the query to create the new table\n",
    "        query_job = self.client.query(sql_query)\n",
    "        query_job.result()  # Wait for the query to finish\n",
    "        print(f\"Table created successfully: {self.project_id}.{self.dataset_name}.{table_name}\")\n",
    "        self.export_table_to_gcs(bucket_name = \"{}-{}\".format(self.project_id, table_name), target_table = table_name, shard_name = \"df\")\n",
    "        self.download_bucket_from_gcs(bucket_name = \"{}-{}\".format(self.project_id, table_name), output_dir_name = table_name)\n",
    "        self.decompress_gz_files(target_directory = os.path.join(self.storage_bucket_compresssed_path, table_name), output_dir_name = table_name)\n",
    "\n",
    "    @overrides\n",
    "    def identify_proxies(self, contracts_list):\n",
    "        \"\"\"\n",
    "        Orchestrates the process to classify each contract in the given list as active proxy or non-proxy.\n",
    "        The process includes creating necessary tables, collecting delegatecall traces, identifying proxy-logic pairs,\n",
    "        and evaluating the active proxy status. This function cannot detect inactive proxy contracts.\n",
    "    \n",
    "        Args:\n",
    "\n",
    "            contracts (list): List of Ethereum contract addresses in hexadecimal format to be classified.\n",
    "    \n",
    "        Steps:\n",
    "        1. Checks for the existence of the specified dataset within the project, and creates it if it does not exist.\n",
    "        2. Constructs a table specifically for the provided contract addresses, ensuring to overwrite existing data if necessary.\n",
    "        3. Collects full traces of delegatecalls from the contracts, which are essential for identifying proxy behaviors.\n",
    "        4. Identifies pairs of proxy and logic (or implementation) contracts from the delegatecall traces, highlighting active communication.\n",
    "        5. Evaluates the active proxy status of each contract, marking them as either active or not based on the identified interactions.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Ensure the required dataset exists; create it if it doesn't.\n",
    "        if not self.dataset_exists():\n",
    "            self.create_dataset()\n",
    "    \n",
    "        # Construct the contracts table from the provided addresses, with an option to override existing data.\n",
    "        self.create_contracts_table(contracts_list,\n",
    "                                    self.contracts_table_name,\n",
    "                                    override=True)\n",
    "    \n",
    "        # Aggregate delegatecall traces from the constructed contracts table to analyze contract interactions.\n",
    "        self.aggregate_delegatecall_traces(self.contracts_table_name,\n",
    "                                           self.contracts_delegate_trace_table_name,\n",
    "                                           override=True)\n",
    "        \n",
    "        # Identify active proxy and their corresponding logic or implementation contracts from the aggregated traces.\n",
    "        self.identify_proxy_logic_pairs(self.contracts_delegate_trace_table_name,\n",
    "                                        self.contracts_table_name,\n",
    "                                        self.proxy_logic_pairs_table_name,\n",
    "                                        override=True)\n",
    "        \n",
    "        # Evaluate and update the proxy status of each contract based on identified pairs and interactions.\n",
    "        self.evaluate_active_proxy_status(self.contracts_table_name,\n",
    "                                          self.proxy_logic_pairs_table_name,\n",
    "                                          self.contracts_proxy_status_table_name,\n",
    "                                          override=True)\n",
    "        \n",
    "        self.download_active_proxy_contracts(self.contracts_proxy_status_table_name, self.active_proxy_contracts_table_name, override = True)\n",
    "\n",
    "    @overrides\n",
    "    def identify_proxies_as_of(self, as_of_timestamp=Utils.STUDY_END_DATE):\n",
    "        \"\"\"\n",
    "        Identifies all the active proxy contracts as of a specified timestamp and records their statuses in a dataset.\n",
    "    \n",
    "        This function systematically builds and updates a dataset with contract data relevant for identifying\n",
    "        proxy contracts as of a given date. It integrates multiple data collection and\n",
    "        processing steps to produce a comprehensive view of proxy contracts as af certain timestamp.\n",
    "    \n",
    "        Parameters:\n",
    "            as_of_timestamp (str, optional): A string representation of the timestamp, formatted as 'YYYY-MM-DD',\n",
    "                                             that defines the upper limit for data collection and processing. \n",
    "                                             Defaults to '2022-09-01' which replicates the data for our recent publication.\n",
    "    \n",
    "        Workflow:\n",
    "            1. Checks for the existence of the specified dataset, creating it if it does not exist.\n",
    "            2. Collects all deployed contract addresses up to the specified timestamp and stores them in a table.\n",
    "            3. Aggregates delegate call traces from these contracts, focusing on their interactions up to the specified timestamp.\n",
    "            4. Identifies and records pairs of proxy and logic/implementation contracts based on the collected traces.\n",
    "            5. Evaluates and records each contract's proxy status (active or inactive) in the final table.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Create a new dataset if it does not already exist\n",
    "        if not self.dataset_exists():\n",
    "            self.create_dataset()\n",
    "    \n",
    "        # Collect all the unique deployed contract addresses as of the given timestamp\n",
    "        self.collect_contract_addresses_as_of(self.contracts_table_name,\n",
    "                                              as_of_timestamp,\n",
    "                                              override=True)\n",
    "    \n",
    "        # Collect all contracts' full traces where contracts at least delegated once to another address as of the specified timestamp\n",
    "        self.aggregate_delegatecall_traces(self.contracts_table_name,\n",
    "                                           self.contracts_delegate_trace_table_name,\n",
    "                                           as_of_timestamp,\n",
    "                                           override=True)\n",
    "        \n",
    "        # Detect all the proxy and logic/implementation contracts pairs for each proxy contract that actively communicated with its implementation contracts\n",
    "        self.identify_proxy_logic_pairs(self.contracts_delegate_trace_table_name,\n",
    "                                        self.contracts_table_name,\n",
    "                                        self.proxy_logic_pairs_table_name,\n",
    "                                        override=True)\n",
    "        \n",
    "        # Flag the given list of contracts with their proxy status i.e., either active or inactive\n",
    "        self.evaluate_active_proxy_status(self.contracts_table_name,\n",
    "                                          self.proxy_logic_pairs_table_name, \n",
    "                                          self.contracts_proxy_status_table_name,\n",
    "                                          override=True)\n",
    "\n",
    "        self.download_active_proxy_contracts(self.contracts_proxy_status_table_name, self.active_proxy_contracts_table_name, override = True)\n",
    "        \n",
    "        # self.export_table_to_gcs(bucket_name = \"{}-{}\".format(self.project_id, self.proxy_logic_pairs_table_name), target_table = self.proxy_logic_pairs_table_name, shard_name = \"df\")\n",
    "        # self.download_bucket_from_gcs(bucket_name = \"{}-{}\".format(self.project_id, self.proxy_logic_pairs_table_name), output_dir_name = self.proxy_logic_pairs_table_name)\n",
    "        # self.decompress_gz_files(target_directory = os.path.join(self.storage_bucket_compresssed_path, self.proxy_logic_pairs_table_name), output_dir_name = self.proxy_logic_pairs_table_name)        \n",
    "    \n",
    "    def load_active_proxy_logic_pairs(self):\n",
    "        df_active_proxy_logic_pairs = Utils.multicore_read_csv(os.path.join(self.storage_bucket_decompresssed_path, self.proxy_logic_pairs_table_name), num_cores=40)\n",
    "        if len(df_active_proxy_logic_pairs) == 0:\n",
    "            raise (\"please first identify active proxy logic pairs\")\n",
    "        else:\n",
    "            return df_active_proxy_logic_pairs.groupby('from_address')['to_address'].apply(list).to_dict()\n",
    "    \n",
    "    \n",
    "    def is_proxy(self, address):\n",
    "        if address in self.active_proxy_logic_pairs:\n",
    "            return True, self.active_proxy_logic_pairs[address]\n",
    "        else:\n",
    "            return False, []\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ade39c-23ea-4eb0-a4ff-e0fe4ea06bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "%run ./BaseUPCDetector.ipynb\n",
    "%run ./Proxy.ipynb\n",
    "%run ./DUPDetector.ipynb\n",
    "%run ./DynamicProxyDetector.ipynb\n",
    "%run ./BytecodeDecompiler.ipynb\n",
    "%run ./ESUPDetector.ipynb\n",
    "%run ./GoogleClient.ipynb\n",
    "%run ./Utils.ipynb\n",
    "%run ./SMUPDetector.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "078acbb1-e78a-4f89-ad25-ab3ad04ac946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigquery json key path: lateral-command-433401-d4-89aa899f9420.json\n",
      "bigquery project id: lateral-command-433401-d4\n",
      "bigquery proxy detector : storage_dynamic_proxy_detector\n",
      "bigquery bytecode dataset: storage_bytecodes\n",
      "panoramix decompiler timeout: 3600 seconds\n",
      "storage path: /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage\n",
      "data collection start date: 2015-01-01\n",
      "data collection end date: 2022-09-01\n",
      "bytecode decompiler output dir: /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/decompiled-bytecodes\n",
      "number of processing cores: 60\n"
     ]
    }
   ],
   "source": [
    "print('bigquery json key path:', Utils.BQ_KEY_PATH)\n",
    "print('bigquery project id:', Utils.BQ_PROJECT_ID)\n",
    "print('bigquery proxy detector :', Utils.BQ_STORAGE_PROXY_DETECTOR)\n",
    "print('bigquery bytecode dataset:', Utils.BQ_STORAGE_BYTECODES)\n",
    "\n",
    "total_decompilation_time = 0\n",
    "\n",
    "print(\"panoramix decompiler timeout: {} seconds\".format(Utils.DECOMPILER_TIMEOUT))\n",
    "print(\"storage path: {}\".format(Utils.DATA_DIR))\n",
    "print(\"data collection start date: {}\".format(Utils.STUDY_START_DATE))\n",
    "print(\"data collection end date: {}\".format(Utils.STUDY_END_DATE))\n",
    "print(\"bytecode decompiler output dir: {}\".format(Utils.DECOMPILER_OUTPUT_DIR))\n",
    "print(\"number of processing cores: {}\".format(Utils.CORE_COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7185b7-91e3-4335-bddf-55b840a34e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials set from: genuine-arena-433215-n1-0ce4684bdeef.json\n",
      "Operating within project: genuine-arena-433215-n1, dataset: storage_dynamic_proxy_detector\n",
      "Created new directory: /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage\n",
      "Created new directory: /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-compressed\n",
      "Created new directory: /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-decompressed\n",
      "BigQuery client is initialized.\n",
      "Created dataset genuine-arena-433215-n1.storage_dynamic_proxy_detector\n",
      "Existing table dropped: genuine-arena-433215-n1.storage_dynamic_proxy_detector.df-contracts\n",
      "Table created successfully: genuine-arena-433215-n1.storage_dynamic_proxy_detector.df-contracts\n",
      "Existing table dropped: genuine-arena-433215-n1.storage_dynamic_proxy_detector.df-contracts-delegate-traces-table\n",
      "Table created successfully: genuine-arena-433215-n1.storage_dynamic_proxy_detector.df-contracts-delegate-traces-table\n",
      "Existing table dropped: genuine-arena-433215-n1.storage_dynamic_proxy_detector.df-proxy-logic-pairs\n",
      "Table created successfully: genuine-arena-433215-n1.storage_dynamic_proxy_detector.df-proxy-logic-pairs\n",
      "Bucket genuine-arena-433215-n1-df-proxy-logic-pairs created successfully.\n",
      "Created new directory: /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-compressed/df-proxy-logic-pairs\n",
      "Start downloading the bucket genuine-arena-433215-n1-df-proxy-logic-pairs content to /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-compressed/df-proxy-logic-pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:00, 92758.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new directory: /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-decompressed/df-proxy-logic-pairs\n",
      "Start unziping the files under /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-compressed/df-proxy-logic-pairs content to /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-decompressed/df-proxy-logic-pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 23/23 [00:12<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing table dropped: genuine-arena-433215-n1.storage_dynamic_proxy_detector.df-contracts-proxy-status\n",
      "Table created successfully: genuine-arena-433215-n1.storage_dynamic_proxy_detector.df-contracts-proxy-status\n",
      "Existing table dropped: genuine-arena-433215-n1.storage_dynamic_proxy_detector.df-active-proxy-contracts\n",
      "Table created successfully: genuine-arena-433215-n1.storage_dynamic_proxy_detector.df-active-proxy-contracts\n",
      "Bucket genuine-arena-433215-n1-df-active-proxy-contracts created successfully.\n",
      "Created new directory: /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-compressed/df-active-proxy-contracts\n",
      "Start downloading the bucket genuine-arena-433215-n1-df-active-proxy-contracts content to /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-compressed/df-active-proxy-contracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:00, 3601.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new directory: /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-decompressed/df-active-proxy-contracts\n",
      "Start unziping the files under /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-compressed/df-active-proxy-contracts content to /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-decompressed/df-active-proxy-contracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 12/12 [00:08<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 23 CSV files from /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-decompressed/df-proxy-logic-pairs/ directory ...\n",
      "proxy detector init time: 1209.5642728805542 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initialize Proxy Detector. \n",
    "To initialize the Proxy Detector for the first time, set the init parameter to True. \n",
    "This will trigger the identification and download of active proxy contracts. \n",
    "Once the initial download is complete, change the init parameter to False for subsequent runs. \n",
    "This prevents re-downloading the proxy contracts and instead reads the already stored objects from disk.\n",
    "\"\"\"\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "# if you collect the dataset once, sw init parameter to False so as to not recollect the dataset.\n",
    "proxy_detector = DynamicProxyDetector(Utils.BQ_KEY_PATH, Utils.BQ_PROJECT_ID, Utils.BQ_STORAGE_PROXY_DETECTOR, init=True)\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time  # Calculate the difference\n",
    "print(f\"proxy detector init time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7faedb0e-c8d8-4a36-a3df-84c00b4a0481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials set from: genuine-arena-433215-n1-0ce4684bdeef.json\n",
      "Operating within project: genuine-arena-433215-n1, dataset: storage_bytecodes\n",
      "BigQuery client is initialized.\n",
      "Created dataset genuine-arena-433215-n1.storage_bytecodes\n",
      "Existing table dropped: genuine-arena-433215-n1.storage_bytecodes.df-all-contracts-bytecodes\n",
      "Table created successfully: genuine-arena-433215-n1.storage_bytecodes.df-all-contracts-bytecodes\n",
      "Existing table dropped: genuine-arena-433215-n1.storage_bytecodes.df-all-contracts-asof\n",
      "Table created successfully: genuine-arena-433215-n1.storage_bytecodes.df-all-contracts-asof\n",
      "Existing table dropped: genuine-arena-433215-n1.storage_bytecodes.df-all-distinct-bytecodes-hashes\n",
      "Table created successfully: genuine-arena-433215-n1.storage_bytecodes.df-all-distinct-bytecodes-hashes\n",
      "Bucket genuine-arena-433215-n1-df-all-distinct-bytecodes-hashes created successfully.\n",
      "Created new directory: /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-compressed/df-all-distinct-bytecodes-hashes\n",
      "Start downloading the bucket genuine-arena-433215-n1-df-all-distinct-bytecodes-hashes content to /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-compressed/df-all-distinct-bytecodes-hashes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "306it [00:00, 803667.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new directory: /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-decompressed/df-all-distinct-bytecodes-hashes\n",
      "Start unziping the files under /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-compressed/df-all-distinct-bytecodes-hashes content to /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-decompressed/df-all-distinct-bytecodes-hashes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 306/306 [05:19<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing table dropped: genuine-arena-433215-n1.storage_bytecodes.df-all-contracts-bytecode-hashes\n",
      "Table created successfully: genuine-arena-433215-n1.storage_bytecodes.df-all-contracts-bytecode-hashes\n",
      "Bucket genuine-arena-433215-n1-df-all-contracts-bytecode-hashes created successfully.\n",
      "Created new directory: /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-compressed/df-all-contracts-bytecode-hashes\n",
      "Start downloading the bucket genuine-arena-433215-n1-df-all-contracts-bytecode-hashes content to /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-compressed/df-all-contracts-bytecode-hashes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113it [00:00, 325296.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new directory: /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-decompressed/df-all-contracts-bytecode-hashes\n",
      "Start unziping the files under /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-compressed/df-all-contracts-bytecode-hashes content to /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-decompressed/df-all-contracts-bytecode-hashes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [01:29<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new directory: /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/decompiled-bytecodes\n",
      "Reading 113 CSV files from /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-decompressed/df-all-contracts-bytecode-hashes/ directory ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 306 CSV files from /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-decompressed/df-all-distinct-bytecodes-hashes/ directory ...\n",
      "bytecode decompiler init time: 815.7993123531342 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initialize Bytecode Decompiler. \n",
    "To initialize the Bytecode Decompiler for the first time, set the init parameter to True.\n",
    "This will trigger the identification and download of bytecode objects. \n",
    "Once the initial download is complete, change the init parameter to False for subsequent runs. \n",
    "This prevents re-downloading bytecodes and instead reads the already stored objects from disk.\n",
    "\"\"\"\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "# if you collect the dataset once, sw init parameter to False so as to not recollect the dataset.\n",
    "bytecode_decompiler = BytecodeDecompiler(GoogleClient(Utils.BQ_KEY_PATH, Utils.BQ_PROJECT_ID, Utils.BQ_STORAGE_BYTECODES), init=True)\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time  # Calculate the difference\n",
    "print(f\"bytecode decompiler init time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417aa768-7809-431d-87c0-ca1a5b2bd3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials set from: lateral-command-433401-d4-89aa899f9420.json\n",
      "Operating within project: lateral-command-433401-d4, dataset: storage_dynamic_proxy_detector\n",
      "BigQuery client is initialized.\n",
      "Reading 23 CSV files from /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-decompressed/df-proxy-logic-pairs/ directory ...\n"
     ]
    }
   ],
   "source": [
    "# init is set to false for next runs\n",
    "proxy_detector = DynamicProxyDetector(Utils.BQ_KEY_PATH, Utils.BQ_PROJECT_ID, Utils.BQ_STORAGE_PROXY_DETECTOR, init=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c782a5b-4823-4aab-9306-ccbf1c32be6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials set from: lateral-command-433401-d4-89aa899f9420.json\n",
      "Operating within project: lateral-command-433401-d4, dataset: storage_bytecodes\n",
      "BigQuery client is initialized.\n",
      "Reading 113 CSV files from /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-decompressed/df-all-contracts-bytecode-hashes/ directory ...\n",
      "Reading 306 CSV files from /home/local/SAIL/amir/Projects/ReleaseEngineering-V2/replication_package/storage/buckets-decompressed/df-all-distinct-bytecodes-hashes/ directory ...\n"
     ]
    }
   ],
   "source": [
    "# init is set to false for next runs\n",
    "bytecode_decompiler = BytecodeDecompiler(GoogleClient(Utils.BQ_KEY_PATH, Utils.BQ_PROJECT_ID, Utils.BQ_STORAGE_BYTECODES), init=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd5732c7-6277-4088-b7a3-03556ce97ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_upc_batch(input_contract_addresses, beacon_dataset_name, early_stop = False):\n",
    "    bytecode_decompiler.reload_decompiled_bytecodes()\n",
    "\n",
    "    print('number of contracts in queue:', len(input_contract_addresses))\n",
    "    bytecode_decompiler.total_decompilation_time = 0\n",
    "\n",
    "    proxy_decomp_time = decompile_contracts_in_parallel(input_contract_addresses, bytecode_decompiler.contracts_bytecodes_hash, bytecode_decompiler.distinct_bytecodes_hash, bytecode_decompiler.decompiler_output)\n",
    "    print('total decompilation time for proxy contracts:', proxy_decomp_time)\n",
    "\n",
    "    bytecode_decompiler.reload_decompiled_bytecodes()\n",
    "    \n",
    "    scheduled_contracts_for_esup_detector = dict()\n",
    "    scheduled_contracts_for_dup_detector = dict()\n",
    "    \n",
    "    results = []\n",
    "    sw10 = False\n",
    "    for con_addr in tqdm(input_contract_addresses):\n",
    "        if True: \n",
    "            smup_detector = SMUPDetector()\n",
    "            sw10 = False\n",
    "            \n",
    "            proxy = Proxy(con_addr, proxy_detector.is_proxy(con_addr)[1])\n",
    "            proxy.is_proxy_dynamic_detector = proxy_detector.is_proxy(con_addr)[0]\n",
    "            \n",
    "            if not proxy.is_proxy_dynamic_detector:\n",
    "                proxy.is_upgradeability_proxy()\n",
    "                results.append(proxy)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                proxy_decompiled_bytecode_path = bytecode_decompiler.decompile_contract(con_addr, 1)\n",
    "            except Exception as e:\n",
    "                proxy.decompilation_status = \"FAILURE:BYTECODE NOT FOUND\"\n",
    "                proxy.bytecode_hash = \"NOT FOUND\"\n",
    "            \n",
    "            if proxy_decompiled_bytecode_path and proxy_decompiled_bytecode_path.find(\"Failure\") < 0:\n",
    "                proxy.decompilation_status = \"SUCCESS\"\n",
    "                proxy.bytecode_hash = bytecode_decompiler.contracts_bytecodes_hash[con_addr]\n",
    "                detector = smup_detector.is_upc(con_addr, open(proxy_decompiled_bytecode_path, 'r'), bytecode_decompiler.distinct_bytecodes_hash[bytecode_decompiler.contracts_bytecodes_hash[con_addr]])\n",
    "                for res in detector:\n",
    "                    print(res)\n",
    "                    relevant_delegatecall = RelevantDelegatecall()\n",
    "                    \n",
    "                    if res[1].find(\"NON-UPC:NO RELEVANT DELEGATECALL\")>=0:\n",
    "                        if proxy_detector.is_proxy(con_addr)[0]:\n",
    "                            proxy.is_upc = \"FAILURE: DELEGATE NOT FOUND\"\n",
    "                            proxy.is_proxy_static_detector = False\n",
    "                        else:\n",
    "                            proxy.is_upc = False\n",
    "                            proxy.is_proxy_static_detector = False\n",
    "                            \n",
    "                    elif res[1].find(\"NON-UPC:FORWARDER PROXY\")>=0:\n",
    "                        proxy.is_upc = False\n",
    "                        proxy.is_proxy_static_detector = True\n",
    "                        \n",
    "                    elif res[1].find('SMUP') > 0:\n",
    "                        proxy.is_proxy_static_detector = True\n",
    "                        proxy.is_upc = True\n",
    "                        relevant_delegatecall.proxy_func_loc = res[0][2]\n",
    "                        relevant_delegatecall.delegatecall_line = res[0][1]\n",
    "                        relevant_delegatecall.delegatecall_loc = res[0][0]\n",
    "                        relevant_delegatecall.proxy_func_sig = res[0][3]\n",
    "                        relevant_delegatecall.is_upc = True\n",
    "                        relevant_delegatecall.upg_ref_design = \"SMUP\"\n",
    "                        relevant_delegatecall.impact_variables = res[3]\n",
    "                        contract = Contract(con_addr, list(res[2]), bytecode_decompiler.contracts_bytecodes_hash[con_addr], \"PROXY\", \"SUCCESS\")\n",
    "                        relevant_delegatecall.add_contract(contract)\n",
    "                        proxy.add_delegatecall(relevant_delegatecall)\n",
    "                        \n",
    "                    elif res[1] == 'DUP Detector':\n",
    "                        proxy.is_proxy_static_detector = True\n",
    "                        relevant_delegatecall.proxy_func_loc = res[0][2]\n",
    "                        relevant_delegatecall.delegatecall_line = res[0][1]\n",
    "                        relevant_delegatecall.delegatecall_loc = res[0][0]\n",
    "                        relevant_delegatecall.proxy_func_sig = res[0][3]\n",
    "                        impact_var_slots = []\n",
    "                        for slot in list(set(smup_detector.impact_var_slots)):\n",
    "                            if slot.find(\"0x\") == 0 and len(slot) > 8:\n",
    "                                impact_var_slots.append(slot[4:len(slot)-4])\n",
    "                            else:\n",
    "                                impact_var_slots.append(slot)\n",
    "                        \n",
    "                        relevant_delegatecall.impact_variables = impact_var_slots\n",
    "                        relevant_delegatecall.flag = \"DUP:{}\".format(\"MAPPING\" if res[4] else \"\")\n",
    "                        proxy.add_delegatecall(relevant_delegatecall)\n",
    "                        scheduled_contracts_for_dup_detector[con_addr] = proxy\n",
    "                        sw10 = True\n",
    "\n",
    "                    elif res[1].find(\"ESUP Detector\") >= 0:\n",
    "                        proxy.is_proxy_static_detector = True\n",
    "                        relevant_delegatecall.proxy_func_loc = res[0][2]\n",
    "                        relevant_delegatecall.delegatecall_line = res[0][1]\n",
    "                        relevant_delegatecall.delegatecall_loc = res[0][0]\n",
    "                        relevant_delegatecall.proxy_func_sig = res[0][3]\n",
    "                        relevant_delegatecall.flag = \"ESUP\"\n",
    "                        proxy.add_delegatecall(relevant_delegatecall)\n",
    "                        scheduled_contracts_for_esup_detector[con_addr] = proxy\n",
    "                        sw10=True\n",
    "                        \n",
    "                    else:\n",
    "                        relevant_delegatecall.proxy_func_loc = res[0][2]\n",
    "                        relevant_delegatecall.delegatecall_line = res[0][1]\n",
    "                        relevant_delegatecall.delegatecall_loc = res[0][0]\n",
    "                        relevant_delegatecall.proxy_func_sig = res[0][3]\n",
    "                        relevant_delegatecall.is_upc = res[1]\n",
    "                        proxy.add_delegatecall(relevant_delegatecall)\n",
    "                        proxy.is_proxy_static_detector = True   \n",
    "                        \n",
    "            elif proxy_decompiled_bytecode_path and proxy_decompiled_bytecode_path.find(\"Failure\") >= 0:\n",
    "                proxy.bytecode_hash = bytecode_decompiler.contracts_bytecodes_hash[con_addr]\n",
    "                proxy.decompilation_status = \"FAILURE\"\n",
    "                proxy.is_upc = \"FAILURE:DECOMPILER:PROXY\"\n",
    "                \n",
    "            if not sw10:\n",
    "                proxy.is_upgradeability_proxy()\n",
    "                results.append(proxy)\n",
    "    \n",
    "    # run esup detector to schedued contracts\n",
    "    if len(scheduled_contracts_for_esup_detector.values()) > 0:\n",
    "        print('total number of scheduled contracts', len(scheduled_contracts_for_esup_detector.keys()))\n",
    "        implementations = set()\n",
    "        for contract in scheduled_contracts_for_esup_detector.keys():\n",
    "            implementations.update(proxy_detector.is_proxy(contract)[1])\n",
    "        print('total number of implementations', len(implementations))\n",
    "        google_client = GoogleClient(Utils.BQ_KEY_PATH, Utils.BQ_PROJECT_ID, \"storage_beacons\")\n",
    "        google_client.create_dataset()\n",
    "        \n",
    "        # you can comment the next line if beacons are already collected. this is just to avoid recollecting and running queries on Bigquery.\n",
    "        # google_client.collect_and_download_beacons_contracts(list(implementations), beacon_dataset_name, override= True)\n",
    "        df_beacons = Utils.multicore_read_csv(os.path.join(google_client.storage_bucket_decompresssed_path, beacon_dataset_name), num_cores=Utils.CORE_COUNT)\n",
    "        \n",
    "        print(\"number of collected beacons:\", df_beacons.shape[0])\n",
    "        df_beacons = df_beacons[df_beacons['proxy'].isin(scheduled_contracts_for_esup_detector)]\n",
    "        print(df_beacons.shape, df_beacons.drop_duplicates([\"proxy\", \"beacon\"]).shape)\n",
    "    \n",
    "        df_beacons = df_beacons[df_beacons['proxy'].isin(set(scheduled_contracts_for_esup_detector.keys()))]\n",
    "        \n",
    "        to_compile = list(df_beacons['beacon'].values)\n",
    "        for addr in list(df_beacons['beacon'].values):\n",
    "            to_compile += proxy_detector.is_proxy(addr)[1]\n",
    "        to_compile = list(set(to_compile))\n",
    "        \n",
    "        beacon_decomp_time = decompile_contracts_in_parallel(to_compile, bytecode_decompiler.contracts_bytecodes_hash, bytecode_decompiler.distinct_bytecodes_hash, bytecode_decompiler.decompiler_output)\n",
    "        print('total decompilation time for beacon contracts:', beacon_decomp_time)\n",
    "          \n",
    "        bytecode_decompiler.reload_decompiled_bytecodes()\n",
    "        \n",
    "        for proxy_addr in tqdm(scheduled_contracts_for_esup_detector.keys()):\n",
    "            target_dependency = df_beacons[df_beacons['proxy'] == proxy_addr]\n",
    "            proxy_obj = scheduled_contracts_for_esup_detector[proxy_addr]\n",
    "            \n",
    "            idx = -1\n",
    "            for _idx, relevant_delegatecall in enumerate(proxy_obj.relevant_delegatecalls):\n",
    "                if relevant_delegatecall.flag == \"ESUP\":\n",
    "                    idx = _idx\n",
    "            \n",
    "            if len(target_dependency) > 0:\n",
    "                target_dependency_addr = target_dependency.iloc[0]['beacon']\n",
    "                imp_get_func_selector = target_dependency.iloc[0]['beacon_input'][:10]\n",
    "                esup_detector = ESUPDetector(imp_get_func_selector, proxy_detector, bytecode_decompiler)\n",
    "                try:\n",
    "                    target_dependency_bytecode_path = bytecode_decompiler.decompile_contract(target_dependency_addr, 1)\n",
    "                    if target_dependency_bytecode_path.find(\"Failure\") < 0:\n",
    "                        res = esup_detector.is_upc(target_dependency_addr, open(target_dependency_bytecode_path, 'r'),'')\n",
    "                        if res and res is not None and res[1].find('UPC:ESUP') >= 0:\n",
    "                            proxy_obj.is_upc = True\n",
    "                            proxy_obj.relevant_delegatecalls[idx].is_upc = True\n",
    "                            proxy_obj.relevant_delegatecalls[idx].upg_ref_design = \"ESUP\"\n",
    "                            proxy_obj.relevant_delegatecalls[idx].impact_variables = res[3]\n",
    "                            contract = Contract(' -> '.join(res[0]), list(res[2]), bytecode_decompiler.contracts_bytecodes_hash[target_dependency_addr], \"TARGET DEPENDENCY\", \"SUCCESS\", imp_get_func_selector)\n",
    "                            proxy_obj.relevant_delegatecalls[idx].add_contract(contract)\n",
    "                            proxy_obj.determine_proxy_label()\n",
    "                        else:\n",
    "                            proxy_obj.is_upc = False\n",
    "                            proxy_obj.relevant_delegatecalls[idx].is_upc = False\n",
    "                            proxy_obj.relevant_delegatecalls[idx].upg_ref_design = \"\"\n",
    "                            contract = Contract(target_dependency_addr, [], bytecode_decompiler.contracts_bytecodes_hash[target_dependency_addr], \"TARGET DEPENDENCY\", \"SUCCESS\", imp_get_func_selector)\n",
    "                            proxy_obj.relevant_delegatecalls[idx].add_contract(contract)\n",
    "                            proxy_obj.determine_proxy_label()\n",
    "                    else:\n",
    "                        contract = Contract(target_dependency_addr, [],bytecode_decompiler.contracts_bytecodes_hash[target_dependency_addr], \"TARGET DEPENDENCY\", \"FAILURE\", imp_get_func_selector)\n",
    "                        proxy_obj.relevant_delegatecalls[idx].add_contract(contract)\n",
    "                        proxy_obj.determine_proxy_label()\n",
    "                except Exception as e:\n",
    "                    contract = Contract(target_dependency_addr, [], \"NOT FOUND\", \"TARGET DEPENDENCY\", \"FAILURE:BYTECODE NOT FOUND\" , imp_get_func_selector)\n",
    "                    proxy_obj.relevant_delegatecalls[idx].add_contract(contract)\n",
    "                    proxy_obj.determine_proxy_label()\n",
    "            else:\n",
    "                if proxy_obj.is_proxy_dynamic_detector:\n",
    "                    print('target dependency not found') # this is not possible if the contract is indeed a esup pattern. our algorithm hit the right external depedency without error using dynamic analysis\n",
    "                else:\n",
    "                    print('inactive esup proxy')\n",
    "            proxy_obj.is_upgradeability_proxy()\n",
    "            results.append(proxy_obj)\n",
    "    \n",
    "    # run dup detector for the scheduled contracts\n",
    "    if len(scheduled_contracts_for_dup_detector.values()) > 0:\n",
    "        print('total number of scheduled contracts', len(scheduled_contracts_for_dup_detector.keys()))\n",
    "        implementations = set()\n",
    "        for contract in scheduled_contracts_for_dup_detector.keys():\n",
    "            implementations.update(proxy_detector.is_proxy(contract)[1])\n",
    "        print('total number of implementations', len(implementations))\n",
    "\n",
    "        to_compile = list(implementations)\n",
    "        for addr in implementations:\n",
    "            to_compile += proxy_detector.is_proxy(addr)[1]\n",
    "        to_compile = list(set(to_compile))\n",
    "        print('total number of decompilation jobs', len(to_compile))\n",
    "\n",
    "        # decompile all the implementation contracts using multi-processing to speed up.\n",
    "        imp_decomp_time = decompile_contracts_in_parallel(to_compile, bytecode_decompiler.contracts_bytecodes_hash, bytecode_decompiler.distinct_bytecodes_hash, bytecode_decompiler.decompiler_output)\n",
    "        print('total decompilation time for implementation contracts:', imp_decomp_time)        \n",
    "        \n",
    "        bytecode_decompiler.reload_decompiled_bytecodes()\n",
    "\n",
    "        # for each proxy that is scheduled.\n",
    "        for proxy_addr in tqdm(scheduled_contracts_for_dup_detector.keys()):\n",
    "            proxy_obj = scheduled_contracts_for_dup_detector[proxy_addr]\n",
    "            \n",
    "            # for each relevant delegatecall that is scheduced for dup detector\n",
    "            for idx, relevant_delegatecall in enumerate(proxy_obj.relevant_delegatecalls):\n",
    "                if relevant_delegatecall.flag.find('DUP')>=0:\n",
    "                    # if the primary impact variable is of type mapping lets check for diamond first\n",
    "                    sw11 = False\n",
    "                    if relevant_delegatecall.flag.find('MAPPING')>=0:\n",
    "                        dup_detector = DUPDetector([], proxy_detector, bytecode_decompiler)\n",
    "                        is_diamond = dup_detector.is_diamond_upc(proxy_addr)\n",
    "                        if is_diamond != False:\n",
    "                            proxy_obj.is_upc = True\n",
    "                            proxy_obj.relevant_delegatecalls[idx].is_upc = True\n",
    "                            proxy_obj.relevant_delegatecalls[idx].upg_ref_design = \"DUP\"\n",
    "                            contract = Contract(is_diamond, [\"diamondCut((address,uint8,bytes4[])[],address,bytes)\"], bytecode_decompiler.contracts_bytecodes_hash[is_diamond], \"DIAMOND CUT\", \"SUCCESS\")\n",
    "                            proxy_obj.relevant_delegatecalls[idx].add_contract(contract)\n",
    "                            proxy_obj.determine_proxy_label()\n",
    "                            sw11 = True\n",
    "                    # if either diamond not found or primary impact variable is not of type mapping check the proxy's impelemeattions for dup style\n",
    "                    if sw11 == False or relevant_delegatecall.flag.find('MAPPING') < 0:\n",
    "                        for imp in proxy_detector.is_proxy(proxy_addr)[1]:\n",
    "                            dup_detector = DUPDetector(proxy_obj.relevant_delegatecalls[idx].impact_variables, proxy_detector, bytecode_decompiler)\n",
    "                            try:\n",
    "                                decompiled_bytecode_path = bytecode_decompiler.decompile_contract(imp, 1)\n",
    "                                if decompiled_bytecode_path.find(\"Failure\") < 0:\n",
    "                                    res = dup_detector.is_upc(imp, open(decompiled_bytecode_path, 'r'), bytecode_decompiler.distinct_bytecodes_hash[bytecode_decompiler.contracts_bytecodes_hash[imp]])\n",
    "                                    if res and res is not None and res[1].find('DUP') > 0:\n",
    "                                        proxy_obj.is_upc = True\n",
    "                                        proxy_obj.relevant_delegatecalls[idx].is_upc = True\n",
    "                                        proxy_obj.relevant_delegatecalls[idx].upg_ref_design = \"DUP\"\n",
    "                                        contract = Contract(' -> '.join(res[0]), list(res[2]), bytecode_decompiler.contracts_bytecodes_hash[imp], \"IMPLEMENTATION\", \"SUCCESS\")\n",
    "                                        proxy_obj.relevant_delegatecalls[idx].add_contract(contract)\n",
    "                                        proxy_obj.determine_proxy_label()\n",
    "                                        if early_stop:\n",
    "                                            break\n",
    "                                    else:\n",
    "                                        contract = Contract(imp, [], bytecode_decompiler.contracts_bytecodes_hash[imp], \"IMPLEMENTATION\", \"SUCCESS\")\n",
    "                                        proxy_obj.relevant_delegatecalls[idx].add_contract(contract)\n",
    "                                        proxy_obj.determine_proxy_label()\n",
    "                                else:\n",
    "                                    contract = Contract(imp, [], bytecode_decompiler.contracts_bytecodes_hash[imp], \"IMPLEMENTATION\", \"FAILURE\")\n",
    "                                    proxy_obj.relevant_delegatecalls[idx].add_contract(contract)\n",
    "                                    proxy_obj.determine_proxy_label()\n",
    "                            except Exception as e:\n",
    "                                contract = Contract(imp, [], \"NOT FOUND\", \"IMPLEMENTATION\", \"FAILURE:BYTECODE NOT FOUND\")\n",
    "                                proxy_obj.relevant_delegatecalls[idx].add_contract(contract)\n",
    "                                proxy_obj.determine_proxy_label()\n",
    "            proxy_obj.is_upgradeability_proxy()\n",
    "            results.append(proxy_obj)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd1ea432-084a-456c-845e-84735137230d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of contracts in groun truth: 920\n"
     ]
    }
   ],
   "source": [
    "df_input_contracts = pd.read_csv(\"../ground truths/GB[1-3] - 920 UPCs & Non-UPCs.csv\")\n",
    "input_contracts = list(df_input_contracts['contract_address'].values)\n",
    "print('number of contracts in groun truth:', len(input_contracts))\n",
    "\n",
    "# Sor the list in place\n",
    "input_contracts.sort()\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "uschunt_results = is_upc_batch(input_contracts, 'df-beacons-uschunt', early_stop = True)\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "\n",
    "total_processing_time = end_time - start_time  # Calculate the difference\n",
    "\n",
    "print(f\"Total processing time: {total_processing_time} seconds\")\n",
    "print(f\"Total decompilation time: {bytecode_decompiler.total_decompilation_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb8790bd-4783-46ef-97c8-dbae8d4904b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of contracts in groun truth: 3177\n"
     ]
    }
   ],
   "source": [
    "df_input_contracts = pd.read_csv(\"../ground truths/GE - 3177 UPCs.csv\")\n",
    "input_contracts = list(df_input_contracts['contract_address'].values)\n",
    "print('number of contracts in groun truth:', len(input_contracts))\n",
    "\n",
    "# Sor the list in place\n",
    "input_contracts.sort()\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "etherscan_results  = is_upc_batch(input_contracts, 'df-beacons-etherscan', early_stop = True)\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "\n",
    "total_processing_time = end_time - start_time  # Calculate the difference\n",
    "\n",
    "print(f\"Total processing time: {total_processing_time} seconds\")\n",
    "print(f\"Total decompilation time: {bytecode_decompiler.total_decompilation_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ffd324-970e-4114-9369-706bbb897790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

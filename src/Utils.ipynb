{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b504c4-1bec-4402-aae9-0aad013cbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import os\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import glob\n",
    "import re\n",
    "import multiprocessing\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae243d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class SQLiteProxyDict:\n",
    "    def __init__(self, db_path, table_name, key_column, value_column, csv_path=None, force_reload=False):\n",
    "        self.db_path = db_path\n",
    "        self.table_name = table_name\n",
    "        self.key_column = key_column\n",
    "        self.value_column = value_column\n",
    "        \n",
    "        db_dir = os.path.dirname(self.db_path)\n",
    "        if db_dir and not os.path.exists(db_dir):\n",
    "            os.makedirs(db_dir, exist_ok=True)\n",
    "\n",
    "        table_exists = False\n",
    "        if os.path.exists(self.db_path):\n",
    "            with self._get_conn() as conn:\n",
    "                cursor = conn.cursor()\n",
    "                cursor.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{self.table_name}';\")\n",
    "                if cursor.fetchone():\n",
    "                    table_exists = True\n",
    "        \n",
    "        if force_reload or not table_exists:\n",
    "            if csv_path:\n",
    "                print(f\"Populating table '{self.table_name}' in database '{self.db_path}' from CSV directory: {csv_path}\")\n",
    "                self._populate_from_csv(csv_path)\n",
    "            elif not table_exists:\n",
    "                 raise LookupError(f\"Database table '{self.table_name}' not found in '{self.db_path}' and no CSV path provided to populate it.\")\n",
    "        \n",
    "    def _get_conn(self):\n",
    "        return sqlite3.connect(self.db_path)\n",
    "\n",
    "    \n",
    "    def _populate_from_csv(self, csv_dir_path): # csv_reader_func is not directly used here anymore for batch loading\n",
    "        print(f\"Reading CSVs from directory: {csv_dir_path} and populating table '{self.table_name}'\")\n",
    "        \n",
    "        if not os.path.isdir(csv_dir_path):\n",
    "            raise ValueError(f\"The specified CSV directory '{csv_dir_path}' is not a valid directory.\")\n",
    "\n",
    "        csv_files = [os.path.join(csv_dir_path, f) for f in os.listdir(csv_dir_path) if f.endswith('.csv')]\n",
    "\n",
    "        if not csv_files:\n",
    "            raise FileNotFoundError(f\"No CSV files found in the directory '{csv_dir_path}'.\")\n",
    "\n",
    "        with self._get_conn() as conn:\n",
    "            # Drop the table if it exists to ensure a clean load, matching 'replace' behavior\n",
    "            conn.execute(f\"DROP TABLE IF EXISTS \\\"{self.table_name}\\\";\")\n",
    "            conn.commit()\n",
    "\n",
    "            first_file = True\n",
    "            for csv_file_path in csv_files:\n",
    "                try:\n",
    "                    # Read one CSV file at a time\n",
    "                    df_chunk = pd.read_csv(csv_file_path)\n",
    "                    \n",
    "                    if self.key_column not in df_chunk.columns or self.value_column not in df_chunk.columns:\n",
    "                        print(f\"Warning: Key column '{self.key_column}' or value column '{self.value_column}' not in {csv_file_path}. Skipping this file for table '{self.table_name}'. Columns found: {list(df_chunk.columns)}\")\n",
    "                        continue\n",
    "\n",
    "                    df_to_load = df_chunk[[self.key_column, self.value_column]].copy()\n",
    "                    # Drop duplicates within the chunk based on key_column.\n",
    "                    # Note: This doesn't handle duplicates across different CSV files if a key appears in multiple files.\n",
    "                    # If global uniqueness is required and keys can span files, a more complex strategy or\n",
    "                    # relying on SQLite's UNIQUE constraint (and handling potential errors) would be needed.\n",
    "                    # For simplicity, we'll keep 'first' within a chunk.\n",
    "                    df_to_load.drop_duplicates(subset=[self.key_column], keep='first', inplace=True)\n",
    "                    \n",
    "                    # Append data to the SQL table\n",
    "                    # 'if_exists' is 'append' because we handle table creation/dropping manually for the first file\n",
    "                    # or ensure it's created before the loop.\n",
    "                    # For the first file, we create the table. For subsequent, we append.\n",
    "                    if first_file:\n",
    "                        df_to_load.to_sql(self.table_name, conn, if_exists='replace', index=False)\n",
    "                        first_file = False\n",
    "                    else:\n",
    "                        df_to_load.to_sql(self.table_name, conn, if_exists='append', index=False)\n",
    "                    \n",
    "                    print(f\"Loaded data from {csv_file_path} into '{self.table_name}'.\")\n",
    "\n",
    "                except pd.errors.EmptyDataError:\n",
    "                    print(f\"Warning: CSV file {csv_file_path} is empty. Skipping.\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {csv_file_path}: {e}. Skipping this file.\")\n",
    "                    continue\n",
    "            \n",
    "            if first_file: # No data was loaded\n",
    "                 # Create an empty table with correct schema if no files had data or all were skipped\n",
    "                print(f\"No data loaded into '{self.table_name}'. Creating an empty table.\")\n",
    "                # We need a schema. This is tricky without at least one valid DataFrame.\n",
    "                # For now, we'll assume if all files are bad/empty, the table might not get created correctly\n",
    "                # or will be empty. A more robust solution might involve defining schema explicitly.\n",
    "                # Let's try to create it with dummy data if no files were processed.\n",
    "                # This part is a bit of a placeholder for robust empty table creation.\n",
    "                # A better way would be to define schema explicitly.\n",
    "                # conn.execute(f\"CREATE TABLE IF NOT EXISTS \\\"{self.table_name}\\\" (\\\"{self.key_column}\\\" TEXT PRIMARY KEY, \\\"{self.value_column}\\\" TEXT);\")\n",
    "\n",
    "                # For now, if no files are processed, the table won't be created by to_sql.\n",
    "                # We will rely on the index creation to fail if the table doesn't exist,\n",
    "                # or handle it by checking if the table exists before creating index.\n",
    "                pass\n",
    "\n",
    "\n",
    "            # Create index after all data is loaded\n",
    "            # Check if table exists before creating index, in case all CSVs were empty/faulty\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{self.table_name}';\")\n",
    "            if cursor.fetchone():\n",
    "                safe_key_column_for_index = self.key_column.replace('\"', '\"\"')\n",
    "                safe_table_name_for_index = self.table_name.replace('\"', '\"\"')\n",
    "                conn.execute(f'CREATE INDEX IF NOT EXISTS \"idx_{safe_table_name_for_index}_{safe_key_column_for_index}\" ON \"{safe_table_name_for_index}\" (\"{safe_key_column_for_index}\");')\n",
    "                print(f\"Index created on '{self.key_column}' for table '{self.table_name}'.\")\n",
    "            else:\n",
    "                print(f\"Table '{self.table_name}' was not created (possibly no valid CSV data). Index creation skipped.\")\n",
    "\n",
    "            conn.commit()\n",
    "        print(f\"Successfully populated table '{self.table_name}' in '{self.db_path}' by processing files individually.\")\n",
    "\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        with self._get_conn() as conn:\n",
    "            cursor = conn.cursor()\n",
    "            # Ensure column names are safe for SQL selection\n",
    "            safe_value_column = f'\"{self.value_column}\"'\n",
    "            safe_key_column = f'\"{self.key_column}\"'\n",
    "            safe_table_name = f'\"{self.table_name}\"'\n",
    "            \n",
    "            query = f\"SELECT {safe_value_column} FROM {safe_table_name} WHERE {safe_key_column} = ?\"\n",
    "            cursor.execute(query, (key,))\n",
    "            row = cursor.fetchone()\n",
    "        \n",
    "        if row:\n",
    "            return row[0]\n",
    "        else:\n",
    "            raise KeyError(f\"Key '{key}' not found in table '{self.table_name}' column '{self.key_column}'.\")\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        with self._get_conn() as conn:\n",
    "            cursor = conn.cursor()\n",
    "            safe_key_column = f'\"{self.key_column}\"'\n",
    "            safe_table_name = f'\"{self.table_name}\"'\n",
    "            \n",
    "            query = f\"SELECT 1 FROM {safe_table_name} WHERE {safe_key_column} = ?\"\n",
    "            cursor.execute(query, (key,))\n",
    "            return cursor.fetchone() is not None\n",
    "            \n",
    "    def get(self, key, default=None):\n",
    "        try:\n",
    "            return self[key]\n",
    "        except KeyError:\n",
    "            return default\n",
    "\n",
    "    def close(self):\n",
    "        # Connections are managed per operation with 'with' statement, so explicit close might not be needed\n",
    "        # unless a persistent connection was maintained.\n",
    "        pass\n",
    "\n",
    "# Make sure this class is defined in Utils.ipynb so %run ./Utils.ipynb makes it available.\n",
    "# Also ensure pandas and os are imported in the cell where this class is defined or earlier in Utils.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb586a-b38c-4eaa-8ca1-0e877ac84ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils: \n",
    "\n",
    "    DATA_DIR = os.path.abspath(\"../storage/\")\n",
    "    DECOMPILER_OUTPUT_DIR = os.path.join(DATA_DIR, 'decompiled-bytecodes')\n",
    "    DECOMPILER_TIMEOUT = 3600\n",
    "    STUDY_START_DATE = \"2015-01-01\"\n",
    "    STUDY_END_DATE = \"2025-05-01\"\n",
    "    \n",
    "    BQ_KEY_PATH = 'cryptoassetanalytics-1e1c1b69e836.json'\n",
    "    BQ_PROJECT_ID = '-'.join(BQ_KEY_PATH.split(\"-\")[:len(BQ_KEY_PATH.split(\"-\")) -1])\n",
    "    BQ_STORAGE_PROXY_DETECTOR = 'storage_dynamic_proxy_detector'\n",
    "    BQ_STORAGE_BYTECODES = 'storage_bytecodes'    \n",
    "    CORE_COUNT = int(multiprocessing.cpu_count() * 0.75) # 80 * .75 = 60\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_directory(directory_path, override= False):\n",
    "        \"\"\"\n",
    "        This function checks if the specified directory exists, deletes it along with its contents if overrides set true,\n",
    "        and if director does not exist it creates an empty directory at the same path.\n",
    "    \n",
    "        Parameters:\n",
    "        directory_path (str): The file path to the directory to be checked and recreated.\n",
    "        \"\"\"\n",
    "        # Check if the directory exists\n",
    "        if os.path.exists(directory_path) and override:\n",
    "            # Remove the directory and all its contents\n",
    "            shutil.rmtree(directory_path)\n",
    "            print(f\"Removed existing directory: {directory_path}\")\n",
    "            \n",
    "            # Recreate the directory\n",
    "            os.makedirs(directory_path)\n",
    "            print(f\"Created new directory: {directory_path}\")\n",
    "        \n",
    "        elif not os.path.exists(directory_path):\n",
    "            # create the directory\n",
    "            os.makedirs(directory_path)\n",
    "            print(f\"Created new directory: {directory_path}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def multicore_read_csv(target_dir, num_cores=CORE_COUNT):\n",
    "        \"\"\"\n",
    "        Read multiple CSV files from the specified directory using multiple cores.\n",
    "\n",
    "        Args:\n",
    "            target_dir (str): The directory containing the CSV files.\n",
    "            num_cores (int): The number of cores to use for reading the files.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A concatenated DataFrame containing data from all CSV files.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If target_dir is not a directory or num_cores is not a positive integer.\n",
    "            FileNotFoundError: If no CSV files are found in the target directory.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(target_dir):\n",
    "            raise ValueError(f\"The specified target directory '{target_dir}' is not a valid directory.\")\n",
    "        \n",
    "        if not isinstance(num_cores, int) or num_cores <= 0:\n",
    "            raise ValueError(\"The number of cores must be a positive integer.\")\n",
    "\n",
    "        # Ensure the target directory path ends with a slash\n",
    "        if not target_dir.endswith('/'):\n",
    "            target_dir += '/'\n",
    "        \n",
    "        batches_path = [batch for batch in glob.glob(target_dir + \"*.csv\")]\n",
    "\n",
    "        if len(batches_path) == 0:\n",
    "            raise FileNotFoundError(f\"No CSV files found in the target directory '{target_dir}'.\")\n",
    "\n",
    "        print('Reading {} CSV files from {} directory ...'.format(len(batches_path), target_dir))\n",
    "        \n",
    "        with Pool(num_cores) as p:\n",
    "            df = pd.concat(p.map(pd.read_csv, batches_path), ignore_index=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_dict(dataframe, col1_key, col2_value):\n",
    "        \"\"\"\n",
    "        Convert two columns of a DataFrame into a dictionary with keys from col1_key and values from col2_value.\n",
    "    \n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): The DataFrame containing the data.\n",
    "            col1_key (str): The column name to use as keys in the dictionary.\n",
    "            col2_value (str): The column name to use as values in the dictionary.\n",
    "    \n",
    "        Returns:\n",
    "            dict: A dictionary with keys from col1_key and values from col2_value.\n",
    "    \n",
    "        Raises:\n",
    "            TypeError: If the input dataframe is not a pandas DataFrame.\n",
    "            ValueError: If col1_key or col2_value are not columns in the DataFrame.\n",
    "        \"\"\"\n",
    "        if not isinstance(dataframe, pd.DataFrame):\n",
    "            raise TypeError(\"The input dataframe must be a pandas DataFrame.\")\n",
    "        \n",
    "        if col1_key not in dataframe.columns:\n",
    "            raise ValueError(f\"Column '{col1_key}' is not in the DataFrame.\")\n",
    "        \n",
    "        if col2_value not in dataframe.columns:\n",
    "            raise ValueError(f\"Column '{col2_value}' is not in the DataFrame.\")\n",
    "    \n",
    "        return dict(zip(dataframe[col1_key], dataframe[col2_value]))\n",
    "\n",
    "    @staticmethod\n",
    "    def escape_ansi(line):\n",
    "        ansi_escape =re.compile(r'(\\x9B|\\x1B\\[)[0-?]*[ -\\/]*[@-~]')\n",
    "        return ansi_escape.sub('', line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

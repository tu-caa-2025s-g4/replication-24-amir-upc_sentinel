{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e3128b1-8ccd-4084-a866-c3d18fb17965",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Utils.ipynb\n",
    "from abc import ABC, abstractmethod\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.storage import Client, transfer_manager\n",
    "import os\n",
    "import shutil\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "class GoogleClient(ABC):\n",
    "    \n",
    "    def __init__(self, json_credentials, project_id, dataset_name, storage_path = Utils.DATA_DIR):\n",
    "        \"\"\"\n",
    "        Initialize the Query class with Google Cloud credentials, project ID, and dataset name.\n",
    "\n",
    "        Args:\n",
    "            json_credentials (str): Path to the Google Cloud JSON credentials file.\n",
    "            project_id (str): Identifier for the Google Cloud project.\n",
    "            dataset_name (str): Name of the dataset where operations will be performed.\n",
    "        \n",
    "        Raises:\n",
    "            FileNotFoundError: If the JSON credentials file does not exist.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Verify if the credentials file exists\n",
    "        if not os.path.exists(json_credentials):\n",
    "            raise FileNotFoundError(f\"The specified credentials file was not found: {json_folders(json_credentials)}\")\n",
    "        \n",
    "        # Set the path to the service account key file\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = json_credentials\n",
    "        print(f\"Credentials set from: {json_credentials}\")\n",
    "        \n",
    "        if not all(isinstance(param, str) and param for param in [project_id, dataset_name]):\n",
    "            raise ValueError(\"contracts_table, project_id, dataset_name, and table_name must be non-empty strings.\")\n",
    "        \n",
    "        self.project_id = project_id\n",
    "        self.dataset_name = dataset_name\n",
    "        \n",
    "        print(f\"Operating within project: {project_id}, dataset: {dataset_name}\")\n",
    "        \n",
    "        self.client = bigquery.Client()\n",
    "        \n",
    "        self.storage_path = os.path.abspath(storage_path)\n",
    "        self.storage_bucket_compresssed_path = os.path.join(self.storage_path, \"buckets-compressed\")\n",
    "        self.storage_bucket_decompresssed_path = os.path.join(self.storage_path, \"buckets-decompressed\") \n",
    "        \n",
    "        Utils.create_directory(self.storage_path)\n",
    "        Utils.create_directory(self.storage_bucket_compresssed_path)\n",
    "        Utils.create_directory(self.storage_bucket_decompresssed_path)\n",
    "        \n",
    "        self.storage_client = storage.Client()\n",
    "        print(f\"BigQuery client is initialized.\")\n",
    "\n",
    "    def set_project_id(self, project_id):\n",
    "        \"\"\"\n",
    "        Sets the project ID for the Query instance, updating the operation context to the specified project.\n",
    "\n",
    "        Args:\n",
    "            project_id (str): The new project ID to be used for future operations within this Query instance.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the project_id is not a non-empty string.\n",
    "        \"\"\"\n",
    "        # Validate the project_id to ensure it is a non-empty string.\n",
    "        if not all(isinstance(param, str) and param for param in [project_id]):\n",
    "            raise ValueError(\"project_id must be a non-empty string.\")\n",
    "        \n",
    "        # Set the project_id attribute to the new value and print a confirmation.\n",
    "        self.project_id = project_id\n",
    "        print(f\"Operating within project: {self.project_id}\")\n",
    "    \n",
    "    def set_dataset(self, dataset_name):\n",
    "        \"\"\"\n",
    "        Sets the dataset name for the Query instance, updating the operation context to the specified dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset_name (str): The new dataset name to be used for future operations within this Query instance.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset_name is not a non-empty string.\n",
    "        \"\"\"\n",
    "        # Validate the dataset_name to ensure it is a non-empty string.\n",
    "        if not all(isinstance(param, right_stages([dataset_name]), str) and param for param in [dataset_name]):\n",
    "            raise ValueError(\"dataset_name must be a non-empty string.\")\n",
    "            \n",
    "        # Set the dataset_name attribute to the new value and print a confirmation.\n",
    "        self.dataset_name = dataset_name\n",
    "        print(f\"Operating within dataset: {self.dataset_name}\")\n",
    "        \n",
    "        \n",
    "    def dataset_exists(self):\n",
    "        \"\"\"\n",
    "        Check if a dataset exists in a project.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if the dataset exists, False otherwise.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset does not exist.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Construct the full dataset ID\n",
    "        full_dataset_id = f\"{self.project_id}.{self.dataset_name}\"\n",
    "\n",
    "        # Check if the dataset exists\n",
    "        try:\n",
    "            dataset = self.client.get_dataset(full_dataset_id, retry=None)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def table_exists(self, table_name):\n",
    "        \"\"\"\n",
    "        Check if a table exists in a dataset under a project.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to check.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the table exists, False otherwise.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Construct the full table ID\n",
    "        full_table_id = f\"{self.project_id}.{self.dataset_name}.{table_name}\"\n",
    "\n",
    "        # Check if the table exists\n",
    "        try:\n",
    "            self.client.get_table(full_table_id)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def create_dataset(self, location='US', override=False):\n",
    "        \"\"\"Create a BigQuery dataset in a specified location.\n",
    "        \n",
    "        Args:\n",
    "            location (str): The location in which to create the dataset, e.g., \"US\".\n",
    "            override (bool): Whether to override the dataset if it already exists.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check if the dataset exists\n",
    "        if self.dataset_exists():\n",
    "            if override:\n",
    "                # Delete the existing dataset\n",
    "                full_dataset_id = f\"{self.project_id}.{self.dataset_name}\"\n",
    "                self.client.delete_dataset(full_dataset_id, delete_contents=True, not_found_ok=True)\n",
    "                print(f\"Deleted existing dataset {full_dataset_id}\")\n",
    "            else:\n",
    "                print(f\"Dataset {self.dataset_name} already exists.\")\n",
    "                return\n",
    "        \n",
    "        # Create a DatasetReference object using the dataset_name\n",
    "        dataset_ref = bigquery.DatasetReference.from_string(self.dataset_name, self.client.project)\n",
    "\n",
    "        # Use the DatasetReference to create a Dataset object\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = self.client.create_dataset(dataset, timeout=30)\n",
    "        print(f\"Created dataset {dataset.project}.{dataset.dataset_id}\")\n",
    "        \n",
    "\n",
    "    def create_contracts_table(self, contract_addr_list, table_name, override=False):\n",
    "        \"\"\"\n",
    "        Create a table in BigQuery from a list of contract addresses, with an option to override existing table.\n",
    "\n",
    "        Args:\n",
    "            contract_addr_list (list): List of contract addresses to include in the table.\n",
    "            table_name (str): Name of the table to create.\n",
    "            override (bool): If True, the existing table with the same name will be overwritten.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If any parameter is missing or incorrect.\n",
    "            AssertionError: If contract_addr_list is not a list or is empty.\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate inputs\n",
    "        if not isinstance(contract_addr_list, list) or not contract_addr_list:\n",
    "            raise AssertionError(\"contract_addr_list must be a non-empty list.\")\n",
    "        if not all(isinstance(addr, str) and addr.startswith('0x') for addr in contract_addr_list):\n",
    "            raise ValueError(\"All addresses in contract_addr_list must be non-empty strings starting with '0x'.\")\n",
    "        if not all(isinstance(param, str) and param for param in [self.project_id, self.dataset_name, table_name]):\n",
    "            raise ValueError(\"project_id, dataset_name, and table_name must be non-empty strings.\")\n",
    "        if not self.dataset_exists():\n",
    "            raise ValueError(f\"The '{self.dataset_name}' dataset does not exist in project '{self.project_id}'.\")\n",
    "\n",
    "        # Construct the full table path\n",
    "        full_table_path = f\"`{self.project_id}.{self.dataset_name}.{table_name}`\"\n",
    "\n",
    "        # If override is True, drop the existing table\n",
    "        if override:\n",
    "            drop_table_query = f\"DROP TABLE IF EXISTS {full_table_path}\"\n",
    "            self.client.query(drop_table_query).result()  # Wait for the query to finish\n",
    "            print(f\"Existing table dropped: {full_table_path}\")\n",
    "\n",
    "        # Remove duplicates from the list\n",
    "        unique_addresses = list(set(contract_addr_list))\n",
    "        num_duplicates = len(contract_addr_list) - len(unique_addresses)\n",
    "        if num_duplicates > 0:\n",
    "            print(f\"Removed {num_duplicates} duplicate addresses from the contract_addr_list.\")\n",
    "\n",
    "        # Format the list of addresses into a string for the SQL query\n",
    "        address_list = ', '.join(f\"'{addr}'\" for addr in unique_addresses)\n",
    "\n",
    "        # Create the SQL query\n",
    "        sql_query = f\"\"\"\n",
    "        CREATE TABLE {full_table_path} AS (\n",
    "          SELECT from_address\n",
    "          FROM UNNEST([{address_list}]) AS from_address\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        # Run the query\n",
    "        query_job = self.client.query(sql_query)\n",
    "        query_job.result()  # Wait for the query to finish\n",
    "        print(f\"Table created successfully: {full_table_path}\")\n",
    "\n",
    "    def collect_contract_addresses_as_of(self, table_name, as_of_timestamp, override=False):\n",
    "        \"\"\"\n",
    "        Collect all distinct contract addresses as of the given timestamp and store it in a specified table.\n",
    "\n",
    "        Args:\n",
    "            asof_timestamp (str): A string representation of the timestamp, formatted as 'YYYY-MM-DD',\n",
    "                                  that defines the upper limit for data collection and processing. \n",
    "                                  Defaults to '2022-09-01' which replicates the data for our recent publication.\n",
    "                                  \n",
    "            table_name (str): Name of the table to store the results.\n",
    "            override (bool): If True, the existing table with the same name will be overwritten.\n",
    "        \"\"\"\n",
    "        if not all(isinstance(param, str) and param for param in [self.project_id, self.dataset_name, as_of_timestamp, table_name]):\n",
    "            raise ValueError(\"contracts_table, project_id, dataset_name, and table_name must be non-empty strings.\")\n",
    "        if not self.dataset_exists():\n",
    "            raise ValueError(f\"The '{self.dataset_name}' dataset does not exist in project '{self.project_id}'.\")\n",
    "\n",
    "        # If override is True, drop the existing table\n",
    "        if override:\n",
    "            full_table_path = f\"{self.project_id}.{self.dataset_name}.{table_name}\"\n",
    "            drop_table_query = f\"DROP TABLE IF EXISTS `{full_table_path}`\"\n",
    "            self.client.query(drop_table_query).result()  # Wait for the query to finish\n",
    "            print(f\"Existing table dropped: {full_table_path}\")\n",
    "    \n",
    "        # Construct the SQL query dynamically\n",
    "        sql_query = f\"\"\"\n",
    "        CREATE TABLE `{self.project_id}.{self.dataset_name}.{table_name}` AS (\n",
    "            SELECT DISTINCT address as from_address\n",
    "            FROM `bigquery-public-data.crypto_ethereum.contracts`\n",
    "            WHERE block_timestamp < TIMESTAMP('{as_of_timestamp}')\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        # Run the query to create the new table\n",
    "        query_job = self.client.query(sql_query)\n",
    "        query_job.result()  # Wait for the query to finish\n",
    "        print(f\"Table created successfully: {self.project_id}.{self.dataset_name}.{table_name}\")\n",
    "      \n",
    "    def hash_contracts_bytecodes(self, table_name, keep = 'latest', override=False):\n",
    "        if not all(isinstance(param, str) and param for param in [self.project_id, self.dataset_name, table_name]):\n",
    "            raise ValueError(\"contracts_table, project_id, dataset_name, and table_name must be non-empty strings.\")\n",
    "        if not self.dataset_exists():\n",
    "            raise ValueError(f\"The '{self.dataset_name}' dataset does not exist in project '{self.project_id}'.\")\n",
    "        # If override is True, drop the existing table\n",
    "        if override:\n",
    "            full_table_path = f\"{self.project_id}.{self.dataset_name}.{table_name}\"\n",
    "            drop_table_query = f\"DROP TABLE IF EXISTS `{full_table_path}`\"\n",
    "            self.client.query(drop_table_query).result()  # Wait for the query to finish\n",
    "            print(f\"Existing table dropped: {full_table_path}\")\n",
    "        order = \"DESC\" if keep == 'latest' else \"ASC\"\n",
    "        # Construct the SQL query dynamically\n",
    "        sql_query = f\"\"\"\n",
    "            CREATE TABLE\n",
    "              `{self.project_id}.{self.dataset_name}.{table_name}` AS (\n",
    "              SELECT\n",
    "                address as from_address,\n",
    "                FARM_FINGERPRINT(bytecode) AS bytecode_hash,\n",
    "                bytecode,\n",
    "                block_timestamp\n",
    "              FROM (\n",
    "                SELECT\n",
    "                  address,\n",
    "                  bytecode,\n",
    "                  block_timestamp,\n",
    "                  ROW_NUMBER() OVER (PARTITION BY address ORDER BY block_timestamp {order}) AS rn\n",
    "                FROM\n",
    "                  `bigquery-public-data.crypto_ethereum.contracts` )\n",
    "              WHERE\n",
    "                rn = 1)\n",
    "        \"\"\"\n",
    "        # Run the query to create the new table\n",
    "        query_job = self.client.query(sql_query)\n",
    "        query_job.result()  # Wait for the query to finish\n",
    "        print(f\"Table created successfully: {self.project_id}.{self.dataset_name}.{table_name}\")\n",
    "\n",
    "    \n",
    "    def collect_and_download_beacons_contracts(self, logic_contracts, table_name, as_of_timestamp = \"2022-09-01\", override=False):\n",
    "        if not self.dataset_exists():\n",
    "            raise ValueError(f\"The '{self.dataset_name}' dataset does not exist in project '{self.project_id}'.\")\n",
    "\n",
    "        beacon_detector_input_table = \"df-beacon-detector-input\"\n",
    "        self.create_contracts_table(logic_contracts, beacon_detector_input_table, override=True)\n",
    "        \n",
    "        delegate_trace_table = \"df-delegatecall-trace-table\"\n",
    "        \n",
    "        if override:\n",
    "            full_table_path = f\"{self.project_id}.{self.dataset_name}.{table_name}\"\n",
    "            drop_table_query = f\"DROP TABLE IF EXISTS `{full_table_path}`\"\n",
    "            self.client.query(drop_table_query).result()  # Execute and wait for the query to complete.\n",
    "            print(f\"Existing table dropped: {full_table_path}\")\n",
    "\n",
    "            full_table_path = f\"{self.project_id}.{self.dataset_name}.{delegate_trace_table}\"\n",
    "            drop_table_query = f\"DROP TABLE IF EXISTS `{full_table_path}`\"\n",
    "            self.client.query(drop_table_query).result()  # Execute and wait for the query to complete.\n",
    "            print(f\"Existing table dropped: {full_table_path}\")\n",
    "\n",
    "        # create a table of all transactions with at least one delegatecall trace\n",
    "        if not self.table_exists(delegate_trace_table):\n",
    "            # Construct the SQL query dynamically\n",
    "            sql_query = f\"\"\"\n",
    "            CREATE TABLE `{self.project_id}.{self.dataset_name}.{delegate_trace_table}` AS (\n",
    "                SELECT *,\n",
    "                REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (REPLACE (trace_address, \"197\", \"ÿ\"), \"196\", \"þ\"), \"195\", \"ý\"), \"194\", \"ü\"), \"193\", \"û\"), \"192\", \"ú\"), \"191\", \"ù\"), \"190\", \"ø\"), \"189\", \"÷\"), \"188\", \"ö\"), \"187\", \"õ\"), \"186\", \"ô\"), \"185\", \"ó\"), \"184\", \"ò\"), \"183\", \"ñ\"), \"182\", \"ð\"), \"181\", \"ï\"), \"180\", \"î\"), \"179\", \"í\"), \"178\", \"ì\"), \"177\", \"ë\"), \"176\", \"ê\"), \"175\", \"é\"), \"174\", \"è\"), \"173\", \"ç\"), \"172\", \"æ\"), \"171\", \"å\"), \"170\", \"ä\"), \"169\", \"ã\"), \"168\", \"â\"), \"167\", \"á\"), \"166\", \"à\"), \"165\", \"ß\"), \"164\", \"Þ\"), \"163\", \"Ý\"), \"162\", \"Ü\"), \"161\", \"Û\"), \"160\", \"Ú\"), \"159\", \"Ù\"), \"158\", \"Ø\"), \"157\", \"×\"), \"156\", \"Ö\"), \"155\", \"Õ\"), \"154\", \"Ô\"), \"153\", \"Ó\"), \"152\", \"Ò\"), \"151\", \"Ñ\"), \"150\", \"Ð\"), \"149\", \"Ï\"), \"148\", \"Î\"), \"147\", \"Í\"), \"146\", \"Ì\"), \"145\", \"Ë\"), \"144\", \"Ê\"), \"143\", \"É\"), \"142\", \"È\"), \"141\", \"Ç\"), \"140\", \"Æ\"), \"139\", \"Å\"), \"138\", \"Ä\"), \"137\", \"Ã\"), \"136\", \"Â\"), \"135\", \"Á\"), \"134\", \"À\"), \"133\", \"¿\"), \"132\", \"¾\"), \"131\", \"½\"), \"130\", \"¼\"), \"129\", \"»\"), \"128\", \"º\"), \"127\", \"¹\"), \"126\", \"¸\"), \"125\", \"·\"), \"124\", \"¶\"), \"123\", \"µ\"), \"122\", \"´\"), \"121\", \"³\"), \"120\", \"²\"), \"119\", \"±\"), \"118\", \"°\"), \"117\", \"¯\"), \"116\", \"®\"), \"115\", \"¬­\"), \"114\", \"«\"), \"113\", \"ª\"), \"112\", \"©\"), \"111\", \"¨\"), \"110\", \"§\"), \"109\", \"¦\"), \"108\", \"¥\"), \"107\", \"¤\"), \"106\", \"£\"), \"105\", \"¢\"), \"104\", \"¡\"), \"103\", \"Ÿ\"), \"102\", \"ž\"), \"101\", \"œ\"), \"100\", \"›\"), \"99\", \"š\"), \"98\", \"™\"), \"97\", \"˜\"), \"96\", \"—\"), \"95\", \"–\"), \"94\", \"•\"), \"93\", \"”\"), \"92\", \"“\"), \"91\", \"’\"), \"90\", \"‘\"), \"89\", \"Ž\"), \"88\", \"Œ\"), \"87\", \"‹\"), \"86\", \"Š\"), \"85\", \"‰\"), \"84\", \"ˆ\"), \"83\", \"‡\"), \"82\", \"†\"), \"81\", \"„\"), \"80\", \"ƒ\"), \"79\", \"€\"), \"78\", \"~\"), \"77\", \"}}\"), \"76\", \"|\"), \"75\", \"{{\"), \"74\", \"z\"), \"73\", \"y\"), \"72\", \"x\"), \"71\", \"w\"), \"70\", \"v\"), \"69\", \"u\"), \"68\", \"t\"), \"67\", \"s\"), \"66\", \"r\"), \"65\", \"q\"), \"64\", \"p\"), \"63\", \"o\"), \"62\", \"n\"), \"61\", \"m\"), \"60\", \"l\"), \"59\", \"k\"), \"58\", \"j\"), \"57\", \"i\"), \"56\", \"h\"), \"55\", \"g\"), \"54\", \"f\"), \"53\", \"e\"), \"52\", \"d\"), \"51\", \"c\"), \"50\", \"b\"), \"49\", \"a\"), \"48\", \"_\"), \"47\", \"^\"), \"46\", \"]\"), \"45\", \"[\"), \"44\", \"Z\"), \"43\", \"Y\"), \"42\", \"X\"), \"41\", \"W\"), \"40\", \"V\"), \"39\", \"U\"), \"38\", \"T\"), \"37\", \"S\"), \"36\", \"R\"), \"35\", \"Q\"), \"34\", \"P\"), \"33\", \"O\"), \"32\", \"N\"), \"31\", \"M\"), \"30\", \"L\"), \"29\", \"K\"), \"28\", \"J\"), \"27\", \"I\"), \"26\", \"H\"), \"25\", \"G\"), \"24\", \"F\"), \"23\", \"E\"), \"22\", \"D\"), \"21\", \"C\"), \"20\", \"B\"), \"19\", \"A\"), \"18\", \"@\"), \"17\", \"?\"), \"16\", \">\"), \"15\", \"=\"), \"14\", \"<\"), \"13\", \";\"), \"12\", \":\"), \"11\", \"/\"), \"10\", \".\"), \"9\", \"-\"), \"8\", \"+\"), \"7\", \"*\"), \"6\", \")\"), \"5\", \"(\"), \"4\", \"&\"), \"3\", \"%\"), \"2\", \"$\"), \"1\", \"#\"), \"0\", \"!\") AS trace_address2\n",
    "                FROM `bigquery-public-data.crypto_ethereum.traces`\n",
    "                WHERE transaction_hash IN (\n",
    "                    SELECT transaction_hash\n",
    "                    FROM `bigquery-public-data.crypto_ethereum.traces`\n",
    "                    WHERE to_address IN (\n",
    "                        SELECT from_address\n",
    "                        FROM `{self.project_id}.{self.dataset_name}.{beacon_detector_input_table}`\n",
    "                    )\n",
    "                    AND trace_type = 'call'\n",
    "                    AND call_type = 'delegatecall'\n",
    "                    AND status != 0\n",
    "                    AND error IS NULL\n",
    "                )\n",
    "            )\n",
    "            \"\"\"\n",
    "            # Run the query to create the new table\n",
    "            query_job = self.client.query(sql_query)\n",
    "            query_job.result()  # Wait for the query to finish\n",
    "            print(f\"Table created successfully: {self.project_id}.{self.dataset_name}.{delegate_trace_table}\")\n",
    "        \n",
    "        # Construct the SQL query dynamically that only return distinct bytecodes along with the bytecode_hash\n",
    "        sql_query = f\"\"\"\n",
    "            CREATE TABLE `{self.project_id}.{self.dataset_name}.{table_name}` AS (\n",
    "                SELECT\n",
    "                  *\n",
    "                FROM (\n",
    "                  SELECT\n",
    "                    *,\n",
    "                    ROW_NUMBER() OVER (PARTITION BY proxy, beacon ORDER BY beacon_trace_timestamp DESC) AS row_num,\n",
    "                  FROM (\n",
    "                    SELECT\n",
    "                      proxy_logic_pairs.proxy,\n",
    "                      proxy_logic_pairs.imp,\n",
    "                      proxy_logic_pairs.delegate_trace_address,\n",
    "                      proxy_logic_pairs.parent_trace_address,\n",
    "                      proxy_logic_pairs.transaction_hash,\n",
    "                      traces.from_address AS beacon_caller,\n",
    "                      traces.to_address AS beacon,\n",
    "                      traces.trace_address AS beacon_trace_address,\n",
    "                      traces.input AS beacon_input,\n",
    "                      traces.output AS beacon_output,\n",
    "                      traces.block_timestamp AS beacon_trace_timestamp,\n",
    "                      proxy_logic_pairs.parent_trace_address2,\n",
    "                      proxy_logic_pairs.delegate_trace_address2,\n",
    "                      traces.trace_address2 AS beacon_trace_address2\n",
    "                    FROM (\n",
    "                      SELECT\n",
    "                        *\n",
    "                      FROM (\n",
    "                        SELECT\n",
    "                          *,\n",
    "                          ROW_NUMBER() OVER (PARTITION BY proxy, imp ORDER BY imp DESC) AS row_num2,\n",
    "                        FROM (\n",
    "                          SELECT\n",
    "                            DISTINCT parent_trace.to_address AS proxy,\n",
    "                            delegate_trace.to_address AS imp,\n",
    "                            parent_trace.trace_address AS parent_trace_address,\n",
    "                            parent_trace.trace_address2 AS parent_trace_address2,\n",
    "                            delegate_trace.trace_address AS delegate_trace_address,\n",
    "                            delegate_trace.trace_address2 AS delegate_trace_address2,\n",
    "                            delegate_trace.transaction_hash AS transaction_hash\n",
    "                          FROM (\n",
    "                            SELECT\n",
    "                              from_address,\n",
    "                              to_address,\n",
    "                              trace_address,\n",
    "                              trace_address2,\n",
    "                              transaction_hash,\n",
    "                              input\n",
    "                            FROM\n",
    "                              `{self.project_id}.{self.dataset_name}.{delegate_trace_table}`\n",
    "                            WHERE\n",
    "                              block_timestamp < TIMESTAMP('{as_of_timestamp}')\n",
    "                              AND to_address IN (\n",
    "                                        SELECT from_address\n",
    "                                        FROM `{self.project_id}.{self.dataset_name}.{beacon_detector_input_table}`\n",
    "                                    ) ) delegate_trace\n",
    "                          INNER JOIN\n",
    "                            `{self.project_id}.{self.dataset_name}.{delegate_trace_table}` parent_trace\n",
    "                          ON\n",
    "                            delegate_trace.transaction_hash = parent_trace.transaction_hash\n",
    "                          WHERE\n",
    "                            SUBSTRING(parent_trace.input, 1, 10) = SUBSTRING(delegate_trace.input, 1, 10)\n",
    "                            AND ( SUBSTRING(delegate_trace.trace_address, 1, LENGTH(parent_trace.trace_address)) = parent_trace.trace_address\n",
    "                              OR (LENGTH(delegate_trace.trace_address) = 1\n",
    "                                AND COALESCE(LENGTH(parent_trace.trace_address),0) = 0))\n",
    "                            AND COALESCE(ARRAY_LENGTH(SPLIT(parent_trace.trace_address, ',')),0) + 1 = ARRAY_LENGTH(SPLIT(delegate_trace.trace_address, ','))))\n",
    "                      WHERE\n",
    "                        row_num2 < 5 ) proxy_logic_pairs\n",
    "                    INNER JOIN\n",
    "                      `{self.project_id}.{self.dataset_name}.{delegate_trace_table}` traces\n",
    "                    ON\n",
    "                      proxy_logic_pairs.transaction_hash = traces.transaction_hash) beacon_traces\n",
    "                  WHERE\n",
    "                    COALESCE(beacon_traces.parent_trace_address2,\"!\") <= beacon_traces.beacon_trace_address2\n",
    "                    AND beacon_traces.beacon_trace_address2 < beacon_traces.delegate_trace_address2\n",
    "                    AND beacon_traces.beacon_output LIKE CONCAT('%', SUBSTRING(beacon_traces.imp, 3), '%')\n",
    "                    AND LENGTH(beacon_traces.beacon_trace_address) = LENGTH(beacon_traces.delegate_trace_address))\n",
    "                WHERE\n",
    "                  row_num = 1\n",
    "            )\n",
    "            \"\"\"\n",
    "        # Run the query to create the new table\n",
    "        query_job = self.client.query(sql_query)\n",
    "        query_job.result()  # Wait for the query to finish\n",
    "        print(f\"Table created successfully: {self.project_id}.{self.dataset_name}.{table_name}\")\n",
    "        self.export_table_to_gcs(bucket_name = \"{}-{}\".format(self.project_id, table_name), target_table = table_name, shard_name = \"df\")\n",
    "        self.download_bucket_from_gcs(bucket_name = \"{}-{}\".format(self.project_id, table_name), output_dir_name = table_name)\n",
    "        self.decompress_gz_files(target_directory = os.path.join(self.storage_bucket_compresssed_path, table_name), output_dir_name = table_name)        \n",
    "    \n",
    "    def download_distinct_bytecode_hashes(self, all_contracts_bytecodes_table, contracts_table, table_name, override = False):\n",
    "        # Validate input parameters to ensure they are non-empty strings.\n",
    "        if not all(isinstance(param, str) and param for param in [all_contracts_bytecodes_table, contracts_table, table_name, self.project_id, self.dataset_name]):\n",
    "            raise ValueError(\"contracts_table, project_id, dataset_name, and table_name must be non-empty strings.\") \n",
    "        # Check if the dataset exists in the project; if not, raise an error.\n",
    "        if not self.dataset_exists():\n",
    "            raise ValueError(f\"The '{self.dataset_name}' dataset does not exist in project '{self.project_id}'.\") \n",
    "        # Check if the bytecode table exists; if not, raise an error.\n",
    "        if not self.table_exists(all_contracts_bytecodes_table):\n",
    "            raise ValueError(f\"The '{all_contracts_bytecodes_table}' table does not exist under project '{self.project_id}.{self.dataset_name}'.\")\n",
    "        # Check if the contracts table exists; if not, raise an error.\n",
    "        if not self.table_exists(contracts_table):\n",
    "            raise ValueError(f\"The '{contracts_table}' table does not exist under project '{self.project_id}.{self.dataset_name}'.\")\n",
    "        # If the override flag is set, drop the existing table specified by `table_name`.\n",
    "        if override:\n",
    "            full_table_path = f\"{self.project_id}.{self.dataset_name}.{table_name}\"\n",
    "            drop_table_query = f\"DROP TABLE IF EXISTS `{full_table_path}`\"\n",
    "            self.client.query(drop_table_query).result()  # Execute and wait for the query to complete.\n",
    "            print(f\"Existing table dropped: {full_table_path}\")\n",
    "        # Construct the SQL query dynamically that only return distinct bytecodes along with the bytecode_hash\n",
    "        sql_query = f\"\"\"\n",
    "            CREATE TABLE `{self.project_id}.{self.dataset_name}.{table_name}` AS (\n",
    "                SELECT\n",
    "                    FARM_FINGERPRINT(bytecode) AS bytecode_hash,\n",
    "                    bytecode\n",
    "                FROM (\n",
    "                    SELECT\n",
    "                        *,\n",
    "                        ROW_NUMBER() OVER (PARTITION BY bytecode ORDER BY block_timestamp DESC) AS rn\n",
    "                    FROM `{self.project_id}.{self.dataset_name}.{all_contracts_bytecodes_table}`\n",
    "                    WHERE from_address IN (\n",
    "                        SELECT from_address\n",
    "                        FROM `{self.project_id}.{self.dataset_name}.{contracts_table}`\n",
    "                    )\n",
    "                )\n",
    "                WHERE rn = 1\n",
    "            )\n",
    "            \"\"\"\n",
    "        # Run the query to create the new table\n",
    "        query_job = self.client.query(sql_query)\n",
    "        query_job.result()  # Wait for the query to finish\n",
    "        print(f\"Table created successfully: {self.project_id}.{self.dataset_name}.{table_name}\")\n",
    "        self.export_table_to_gcs(bucket_name = \"{}-{}\".format(self.project_id, table_name), target_table = table_name, shard_name = \"df\")\n",
    "        self.download_bucket_from_gcs(bucket_name = \"{}-{}\".format(self.project_id, table_name), output_dir_name = table_name)\n",
    "        self.decompress_gz_files(target_directory = os.path.join(self.storage_bucket_compresssed_path, table_name), output_dir_name = table_name)\n",
    "    \n",
    "    def download_contracts_bytecode_hashes(self, all_contracts_bytecodes_table, contracts_table, table_name, override=False):\n",
    "        \"\"\"\n",
    "        Downloads bytecode hashes for a specified list of contracts, and optionally\n",
    "        creates or overrides a table in BigQuery to store these hashes.\n",
    "    \n",
    "        Parameters:\n",
    "        - all_contracts_bytecodes_table (str): The BigQuery table containing bytecodes for all contracts.\n",
    "        - contracts_table (str): The BigQuery table containing a list of specific contracts.\n",
    "        - table_name (str): The name of the new table to create or override in BigQuery.\n",
    "        - override (bool): If True, any existing table with the same name will be dropped before creating a new one.\n",
    "        \"\"\"\n",
    "        # Validate input parameters to ensure they are non-empty strings.\n",
    "        if not all(isinstance(param, str) and param for param in [all_contracts_bytecodes_table, contracts_table, table_name, self.project_id, self.dataset_name]):\n",
    "            raise ValueError(\"All parameters must be non-empty strings.\")\n",
    "        # Check if the dataset exists in the project; if not, raise an error.\n",
    "        if not self.dataset_exists():\n",
    "            raise ValueError(f\"The '{self.dataset_name}' dataset does not exist in project '{self.project_id}'.\")\n",
    "        # Check if the bytecode table exists; if not, raise an error.\n",
    "        if not self.table_exists(all_contracts_bytecodes_table):\n",
    "            raise ValueError(f\"The '{all_contracts_bytecodes_table}' table does not exist under project '{self.project_id}.{self.dataset_name}'.\")\n",
    "        # Check if the contracts table exists; if not, raise an error.\n",
    "        if not self.table_exists(contracts_table):\n",
    "            raise ValueError(f\"The '{contracts_table}' table does not exist under project '{self.project_id}.{self.dataset_name}'.\")\n",
    "        # If the override flag is set, drop the existing table specified by `table_name`.\n",
    "        if override:\n",
    "            full_table_path = f\"{self.project_id}.{self.dataset_name}.{table_name}\"\n",
    "            drop_table_query = f\"DROP TABLE IF EXISTS `{full_table_path}`\"\n",
    "            self.client.query(drop_table_query).result()  # Execute and wait for the query to complete.\n",
    "            print(f\"Existing table dropped: {full_table_path}\")\n",
    "        # Create a new table with unique bytecode hashes for specified contracts using a SQL query.\n",
    "        sql_query = f\"\"\"\n",
    "            CREATE TABLE `{self.project_id}.{self.dataset_name}.{table_name}` AS (\n",
    "                SELECT\n",
    "                    from_address,\n",
    "                    FARM_FINGERPRINT(bytecode) AS bytecode_hash\n",
    "                FROM `{self.project_id}.{self.dataset_name}.{all_contracts_bytecodes_table}`\n",
    "                WHERE from_address IN (\n",
    "                    SELECT from_address\n",
    "                    FROM `{self.project_id}.{self.dataset_name}.{contracts_table}`\n",
    "                )\n",
    "            )\n",
    "            \"\"\"\n",
    "        # Execute the query to create the new table with bytecode hashes.\n",
    "        query_job = self.client.query(sql_query)\n",
    "        query_job.result()  # Wait for the query to complete.\n",
    "        print(f\"Table created successfully: {self.project_id}.{self.dataset_name}.{table_name}\")\n",
    "        \n",
    "        # Export the newly created table to Google Cloud Storage and download it to a local directory.\n",
    "        self.export_table_to_gcs(bucket_name = \"{}-{}\".format(self.project_id, table_name), target_table = table_name, shard_name = \"df\")\n",
    "        self.download_bucket_from_gcs(bucket_name = \"{}-{}\".format(self.project_id, table_name), output_dir_name = table_name)\n",
    "        self.decompress_gz_files(target_directory = os.path.join(self.storage_bucket_compresssed_path, table_name), output_dir_name = table_name)        \n",
    "\n",
    "    def download_bucket_from_gcs(self, bucket_name, output_dir_name, workers=8, max_results=10000):\n",
    "        \"\"\"\n",
    "        Downloads all the files (blobs) from a specified Google Cloud Storage (GCS) bucket into a local directory,\n",
    "        using multiple worker processes for concurrent downloads. Directories are created as needed based on the blob names.\n",
    "    \n",
    "        Parameters:\n",
    "        - bucket_name (str): Name of the GCS bucket from which to download the blobs.\n",
    "        - output_dir_name (str): Local directory path where blobs will be downloaded to. If left empty,\n",
    "          it defaults to the name of the bucket.\n",
    "        - workers (int): Number of worker processes to use for downloading. More workers can increase download speed\n",
    "          but also use more CPU and memory.\n",
    "        - max_results (int): Maximum number of blobs to download. Useful for large buckets or limited system resources.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Check if output directory name is empty and set it to bucket name if true\n",
    "        if len(output_dir_name) == 0:\n",
    "            output_dir_name = bucket_name\n",
    "        \n",
    "        # Attempt to get the specified bucket from the GCS\n",
    "        bucket = self.storage_client.bucket(bucket_name)\n",
    "    \n",
    "        # If the bucket exists, proceed with download\n",
    "        if bucket.exists():\n",
    "            # Ensure the output directory path ends with a slash\n",
    "            # output_dir_name = output_dir_name if output_dir_name.endswith('/') else output_dir_name + \"/\"  \n",
    "            # Compose full path to the output directory\n",
    "            output_dir_path = os.path.join(self.storage_bucket_compresssed_path, output_dir_name)\n",
    "               \n",
    "            # Ensure the directory exists; create it if necessary\n",
    "            Utils.create_directory(output_dir_path, override=True)\n",
    "            \n",
    "            # List all blobs in the bucket up to the specified maximum number of results\n",
    "            blob_names = [blob.name for blob in bucket.list_blobs(max_results=max_results)]\n",
    "            # Print starting message if there are blobs to download\n",
    "            if len(blob_names):\n",
    "                print(f\"Start downloading the bucket {bucket_name} content to {output_dir_path}\")\n",
    "            else:\n",
    "                # If no blobs, return early\n",
    "                return\n",
    "                \n",
    "            # Initiate parallel download of blobs using a process pool\n",
    "            results = transfer_manager.download_many_to_path(\n",
    "                bucket, blob_names, destination_directory=output_dir_path, max_workers=workers\n",
    "            )\n",
    "        \n",
    "            # Iterate through results and handle possible exceptions\n",
    "            for name, result in tqdm(zip(blob_names, results)):\n",
    "                # Check if the result is an exception and handle it\n",
    "                if isinstance(result, Exception):\n",
    "                    print(f\"Failed to download {name} due to exception: {result}\")\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            # Raise an error if the bucket does not exist\n",
    "            raise Exception(f\"Bucket {bucket_name} not found.\")\n",
    "\n",
    "    def export_table_to_gcs(self, bucket_name, target_table, shard_name, compress=True):\n",
    "\n",
    "        \"\"\"\n",
    "        Exports a BigQuery table to Google Cloud Storage (GCS) as a CSV file, optionally compressing it.\n",
    "    \n",
    "        This method ensures that the dataset and table exist, creates a new GCS bucket if needed, \n",
    "        and exports the specified BigQuery table to a CSV file in the bucket. The output files can be \n",
    "        optionally compressed using GZIP.\n",
    "    \n",
    "        Parameters:\n",
    "        - bucket_name (str): The name of the GCS bucket where the file will be stored.\n",
    "        - target_table (str): The name of the table in BigQuery to be exported.\n",
    "        - shard_name (str): The base name for the output files, which will be appended with \n",
    "          wildcards to handle sharding.\n",
    "        - compress (bool): If True, the output files will be compressed using GZIP. Default is True.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the dataset exists; if not, raise an error.\n",
    "        if not self.dataset_exists():\n",
    "            raise ValueError(f\"The '{self.dataset_uuid}' dataset does not exist in project '{self.project_id}'.\")\n",
    "        # Check if the specified table exists within the dataset; if not, raise an error.\n",
    "        if not self.table_exists(target_table):\n",
    "            raise ValueError(f\"The '{target_table}' table does not exist under project '{self.project_id}.{self.dataset_name}'.\")\n",
    "    \n",
    "        # Create a Google Cloud Storage (GCS) bucket if it doesn't already exist.\n",
    "        self.create_gcs_bucket(bucket_name, override = True)\n",
    "    \n",
    "        # Define the destination URI for the output file with optional sharding and compression.\n",
    "        destination_uri = f\"gs://{bucket_name}/{shard_name}-*.csv.gz\"\n",
    "        # Reference the dataset in BigQuery.\n",
    "        dataset_ref = bigquery.DatasetReference(self.project_id, self.dataset_name)\n",
    "        # Reference the target table within the dataset.\n",
    "        table_ref = dataset_ref.table(target_table)\n",
    "        # Configure the job to possibly compress the output files.\n",
    "        job_config = bigquery.job.ExtractJobConfig()\n",
    "        \n",
    "        if compress:\n",
    "            # If compression is enabled, use GZIP.\n",
    "            job_config.compression = bigquery.Compression.GZIP\n",
    "        \n",
    "        # Begin the extraction job to export the table to a CSV file in the bucket.\n",
    "        extract_job = self.client.extract_table(\n",
    "            table_ref,\n",
    "            destination_uri,\n",
    "            # Ensure the location of the job matches the source table's location.\n",
    "            location=\"US\",\n",
    "            job_config=job_config,\n",
    "        )  # API request\n",
    "        extract_job.result()  # Wait for the job to complete and ensure no errors occurred.\n",
    "    \n",
    "    def create_gcs_bucket(self, bucket_name, override=True):\n",
    "        \"\"\"\n",
    "        Creates a Google Cloud Storage (GCS) bucket, optionally overriding an existing bucket.\n",
    "    \n",
    "        This method attempts to create a new bucket with the specified name. If the 'override' parameter\n",
    "        is True and the bucket already exists, the existing bucket will be deleted before creating a new one.\n",
    "        This function will catch exceptions and print error messages if the creation fails.\n",
    "    \n",
    "        Parameters:\n",
    "        - bucket_name (str): The name of the bucket to create.\n",
    "        - override (bool): Whether to delete and recreate the bucket if it already exists. Default is True.\n",
    "        \"\"\"\n",
    "        # If override is enabled and the bucket exists, delete the existing bucket.\n",
    "        if override and self.storage_client.bucket(bucket_name).exists():\n",
    "            self.delete_gcs_bucket(bucket_name)\n",
    "        try:\n",
    "            # Attempt to create a new GCS bucket.\n",
    "            bucket = self.storage_client.create_bucket(bucket_name)\n",
    "            print(f'Bucket {bucket_name} created successfully.')\n",
    "        except Exception as e:\n",
    "            # Handle exceptions that may occur during bucket creation and print an error message.\n",
    "            print(f'Error creating bucket: {str(e)}')\n",
    "        \n",
    "    def delete_gcs_bucket(self, bucket_name):\n",
    "        \"\"\"Deletes a bucket from Google Cloud Storage\n",
    "    \n",
    "        Args:\n",
    "            bucket_name (str): Name of the bucket to delete\n",
    "        \"\"\"\n",
    "        # Get the bucket\n",
    "        bucket = self.storage_client.bucket(bucket_name)\n",
    "    \n",
    "        # Check if the bucket exists\n",
    "        if not bucket.exists():\n",
    "            return\n",
    "        \n",
    "        # Delete the bucket\n",
    "        try:\n",
    "            # deletes all the contents of a given bucket\n",
    "            all_blobs = list(bucket.list_blobs())\n",
    "            if len(all_blobs) >0:\n",
    "                for blob in all_blobs:\n",
    "                    blob.delete()\n",
    "            \n",
    "            bucket.delete()\n",
    "            print(f\"Bucket {bucket_name} deleted\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete bucket {bucket_name}: {e}\")\n",
    "\n",
    "    def decompress_gz_files(self, target_directory, output_dir_name):\n",
    "        \"\"\"\n",
    "        Decompresses all .gz files found in the specified target_directory.\n",
    "    \n",
    "        Parameters:\n",
    "        target_directory (str): The path to the directory containing .gz files.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(output_dir_name) == 0:\n",
    "            output_dir_name = bucket_name\n",
    "        # output_dir_name = output_dir_name if output_dir_name.endswith('/') else output_dir_name + \"/\"  \n",
    "        output_dir_path = os.path.join(self.storage_bucket_decompresssed_path, output_dir_name)\n",
    "        Utils.create_directory(output_dir_path, override = True)        \n",
    "        \n",
    "        print(\"Start unziping the files under {} content to {}\".format(target_directory, output_dir_path))\n",
    "\n",
    "        # Iterate over all files in the target_directory\n",
    "        for filename in tqdm(os.listdir(target_directory)):\n",
    "            if filename.endswith('.gz'):\n",
    "                # Form the full path to the .gz file\n",
    "                file_path = os.path.join(target_directory, filename)\n",
    "                # Create the destination file path by removing the .gz extension\n",
    "                output_path = os.path.join(output_dir_path, filename[:-3])\n",
    "                \n",
    "                # Decompress the file\n",
    "                with gzip.open(file_path, 'rb') as f_in, open(output_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
